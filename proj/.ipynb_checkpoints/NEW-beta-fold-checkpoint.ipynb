{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:38:04.034634Z",
     "iopub.status.busy": "2025-04-30T13:38:04.034412Z",
     "iopub.status.idle": "2025-04-30T13:38:04.044285Z",
     "shell.execute_reply": "2025-04-30T13:38:04.043787Z",
     "shell.execute_reply.started": "2025-04-30T13:38:04.034616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'se3_transformer.model.layers.convolution' from '/notebooks/SE3Transformer/se3_transformer/model/layers/convolution.py'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from se3_transformer.model.layers.convolution import VersatileConvSE3\n",
    "import importlib\n",
    "importlib.reload(se3_transformer.model.layers.convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T04:20:29.833602Z",
     "iopub.status.busy": "2025-05-02T04:20:29.832997Z",
     "iopub.status.idle": "2025-05-02T04:20:29.838482Z",
     "shell.execute_reply": "2025-05-02T04:20:29.837897Z",
     "shell.execute_reply.started": "2025-05-02T04:20:29.833580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patched sequential forward\n"
     ]
    }
   ],
   "source": [
    "from se3_transformer.model.transformer import Sequential\n",
    "\n",
    "\n",
    "def fixed_sequential_forward(self, input, *args, **kwargs):\n",
    "    print(f\"\\n--- Entering SE3 Sequential Block ---\")\n",
    "    for i, module in enumerate(self):\n",
    "        print(f\"  Input to Module {i} ({type(module).__name__}):\")\n",
    "        if isinstance(input, dict):\n",
    "            for key, val in input.items():\n",
    "                print(f\"    features['{key}'].shape: {val.shape}\")\n",
    "        else:\n",
    "            print(f\"    input shape: {input.shape}\") # Should likely be a dict\n",
    "\n",
    "        input = module(input, *args, **kwargs) # Call the module\n",
    "\n",
    "        print(f\"  Output from Module {i} ({type(module).__name__}):\")\n",
    "        if isinstance(input, dict):\n",
    "            for key, val in input.items():\n",
    "                print(f\"    features['{key}'].shape: {val.shape}\")\n",
    "        else:\n",
    "             print(f\"    output shape: {input.shape}\") # Should likely be a dict\n",
    "        print(\"-\" * 20)\n",
    "    print(f\"--- Exiting SE3 Sequential Block ---\\n\")\n",
    "    return input\n",
    "\n",
    "Sequential.forward = fixed_sequential_forward\n",
    "print(\"patched sequential forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T04:20:31.907858Z",
     "iopub.status.busy": "2025-05-02T04:20:31.907413Z",
     "iopub.status.idle": "2025-05-02T04:20:31.915640Z",
     "shell.execute_reply": "2025-05-02T04:20:31.915072Z",
     "shell.execute_reply.started": "2025-05-02T04:20:31.907841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to monkeypatch LinearSE3.forward with einsum...\n",
      "Successfully patched LinearSE3.forward with einsum.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from typing import Dict\n",
    "\n",
    "# Import the class to patch\n",
    "from se3_transformer.model.layers.linear import LinearSE3\n",
    "\n",
    "print(\"Attempting to monkeypatch LinearSE3.forward with einsum...\")\n",
    "\n",
    "# Store the original method if you might need it later (optional)\n",
    "# original_linear_forward = LinearSE3.forward\n",
    "\n",
    "# Define the patched function using einsum\n",
    "def patched_linear_einsum_forward(self, features: Dict[str, Tensor], *args, **kwargs) -> Dict[str, Tensor]:\n",
    "    \"\"\"\n",
    "    Patched forward method for LinearSE3 using einsum for correct channel transformation.\n",
    "    Input features expected shape: (N, D, C_in)\n",
    "    Weights expected shape: (C_out, C_in)\n",
    "    Output features shape: (N, D, C_out)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Patched LinearSE3.forward (Einsum) ---\")\n",
    "    print(f\"  Input features keys: {list(features.keys())}\")\n",
    "    for degree, tensor in features.items():\n",
    "        has_nan = torch.isnan(tensor).any()\n",
    "        has_inf = torch.isinf(tensor).any()\n",
    "        print(f\"    features['{degree}'].shape: {tensor.shape}, Has NaN: {has_nan}, Has Inf: {has_inf}\")\n",
    "\n",
    "    print(f\"\\n  Weights keys: {list(self.weights.keys())}\")\n",
    "    for degree, weight in self.weights.items():\n",
    "         print(f\"    self.weights['{degree}'].shape: {weight.shape}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    output_features = {}\n",
    "    # Iterate through the degrees defined in the layer's weights\n",
    "    for degree, weight in self.weights.items():\n",
    "        print(f\"  Processing degree '{degree}':\")\n",
    "\n",
    "        if degree not in features:\n",
    "            print(f\"    WARNING: Degree '{degree}' in weights but not found in input features dict. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        feat = features[degree]\n",
    "        # W: (C_out, C_in)\n",
    "        # X: (N, D, C_in)\n",
    "        # Y: (N, D, C_out)\n",
    "        # Einsum: 'ndi,oi->ndo' where W has shape (C_out, C_in) -> oi\n",
    "\n",
    "        print(f\"      Weight shape (oi): {weight.shape}\")\n",
    "        print(f\"      Feature shape (ndi): {feat.shape}\")\n",
    "\n",
    "        # Check compatibility for einsum: weight C_in must match feature C_in\n",
    "        expected_feat_channels = weight.shape[-1] # C_in from weight\n",
    "        actual_feat_channels = feat.shape[-1]     # C_in from feature\n",
    "\n",
    "        if expected_feat_channels != actual_feat_channels:\n",
    "             print(f\"    ERROR: Channel mismatch for einsum! Weight expects C_in={expected_feat_channels}, Feature has C_in={actual_feat_channels}\")\n",
    "             print(f\"    Skipping einsum for degree '{degree}'.\")\n",
    "             continue\n",
    "\n",
    "        try:\n",
    "            # Apply einsum: N=nodes, D=dim, i=in_channels, o=out_channels\n",
    "            print(f\"    Attempting einsum: torch.einsum('ndi,oi->ndo', features['{degree}'], self.weights['{degree}'])\")\n",
    "            # Note the transpose ('oi') assumes weight is (C_out, C_in)\n",
    "            result = torch.einsum('ndi,oi->ndo', feat, weight)\n",
    "            output_features[degree] = result\n",
    "            print(f\"      -> Success! Output shape: {result.shape}\")\n",
    "            # Check for NaN/Inf in output\n",
    "            has_nan_out = torch.isnan(result).any()\n",
    "            has_inf_out = torch.isinf(result).any()\n",
    "            if has_nan_out or has_inf_out:\n",
    "                 print(f\"      WARNING: Output for degree '{degree}' contains NaN: {has_nan_out}, Inf: {has_inf_out}\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"    !!! RuntimeError during einsum for degree '{degree}' !!!\")\n",
    "            print(f\"      Error message: {e}\")\n",
    "            # raise e\n",
    "            print(f\"    Skipping degree '{degree}' after error.\")\n",
    "            continue # Skip to next degree\n",
    "\n",
    "    print(f\"--- End Patched LinearSE3.forward (Einsum) ---\\n\")\n",
    "    return output_features\n",
    "\n",
    "\n",
    "# Apply the patch\n",
    "LinearSE3.forward = patched_linear_einsum_forward\n",
    "print(\"Successfully patched LinearSE3.forward with einsum.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T04:20:34.866816Z",
     "iopub.status.busy": "2025-05-02T04:20:34.866267Z",
     "iopub.status.idle": "2025-05-02T04:20:34.876535Z",
     "shell.execute_reply": "2025-05-02T04:20:34.875978Z",
     "shell.execute_reply.started": "2025-05-02T04:20:34.866798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to monkeypatch VersatileConvSE3.forward...\n",
      "Successfully patched VersatileConvSE3.forward.\n"
     ]
    }
   ],
   "source": [
    "from se3_transformer.runtime.utils import degree_to_dim\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "# Import the specific class and potentially dependencies like Fiber\n",
    "from se3_transformer.model.layers.convolution import VersatileConvSE3, RadialProfile\n",
    "# You might need other imports if your fixed function uses them\n",
    "\n",
    "print(\"Attempting to monkeypatch VersatileConvSE3.forward...\")\n",
    "\n",
    "# 1. Define your new forward function (copy the corrected logic)\n",
    "def fixed_versatile_forward(self, features: Tensor, invariant_edge_feats: Tensor, basis: Optional[Tensor], degree_in: int, degree_out: int):\n",
    "    \"\"\"\n",
    "    Patched forward method for VersatileConvSE3.\n",
    "    Uses einsum for basis contraction and handles scalar case.\n",
    "    Includes contiguity checks.\n",
    "    \"\"\"\n",
    "    num_edges, dim_in, channels_in_feat = features.shape # Get C_in from actual features\n",
    "\n",
    "    # Basic check for empty graph edge case after bidirectionality\n",
    "    if num_edges == 0:\n",
    "        print(\"VersatileConvSE3.forward: Skipping computation for 0 edges.\")\n",
    "        # Need to determine expected output shape based on fiber_out\n",
    "        # This might require inspecting self.fiber_out which should be available\n",
    "        out_dim = self.fiber_out.dim\n",
    "        out_channels = self.channels_out\n",
    "        return torch.zeros(0, out_channels, out_dim, device=features.device, dtype=features.dtype)\n",
    "\n",
    "\n",
    "    # with nvtx.range(f'RadialProfile_Patched'):\n",
    "    # invariant_edge_feats shape is (num_edges, edge_dim)\n",
    "    # Ensure edge_dim matches what RadialProfile expects\n",
    "    \n",
    "    # Access the internal network (assuming it's called 'net')\n",
    "\n",
    "    radial_output = self.radial_func(invariant_edge_feats)\n",
    "\n",
    "    if basis is not None:\n",
    "        # basis_size = basis.shape[1] // dim_in\n",
    "        basis_size = degree_to_dim(min(degree_in, degree_out))\n",
    "        # Basis contraction path (e.g., 1->0, 1->1, 0->1, etc.)\n",
    "        # basis shape is (num_edges, dim_in * basis_size, dim_out)\n",
    "        print(\"------DEBUG VERSFORWARD------\")\n",
    "        print(f\"features passed in shape: {features.shape}\")\n",
    "        print(f\"basis: {basis.shape}\")\n",
    "        print(f\"dim in: {dim_in}\")\n",
    "        print(f\"        radial_output.shape = {radial_output.shape}\")\n",
    "        expected_radial_numel = num_edges * self.channels_out * channels_in_feat * basis_size\n",
    "        print(f\"        expected_radial_numel = {expected_radial_numel} (based on edges={num_edges}, C_out={self.channels_out}, C_in={channels_in_feat}, F={basis_size})\")\n",
    "\n",
    "        dim_out = basis.shape[-1]\n",
    "\n",
    "        # Calculate expected radial output size\n",
    "        # if radial_output.numel() != expected_radial_numel:\n",
    "        #     raise ValueError(\n",
    "        #         f\"RadialProfile output size mismatch (basis path)! \"\n",
    "        #         f\"Expected {expected_radial_numel}, got {radial_output.numel()}. \"\n",
    "        #         f\"Shape: {radial_output.shape}. C_out={self.channels_out}, C_in={channels_in_feat}, F={basis_size}\"\n",
    "        #     )\n",
    "\n",
    "        # Reshape radial_weights for einsum: (E, C_out, C_in, F)\n",
    "        radial_weights_reshaped = radial_output.view(\n",
    "            num_edges, self.channels_out, channels_in_feat, basis_size\n",
    "        ).contiguous() # Add contiguous\n",
    "\n",
    "        # Reshape basis for einsum: (E, D_in, F, D_out)\n",
    "        basis_reshaped = basis.view(num_edges, dim_in, basis_size, dim_out).contiguous() # Add contiguous\n",
    "\n",
    "        # Ensure features are contiguous\n",
    "        features = features.contiguous()\n",
    "\n",
    "        # Perform the correct tensor contraction: nli,noif,nlfk->nok\n",
    "        try:\n",
    "            out = torch.einsum('nli,noif,nlfk->nok', features, radial_weights_reshaped, basis_reshaped)\n",
    "        except RuntimeError as e:\n",
    "             print(\"ERROR during einsum (basis path)!\")\n",
    "             print(f\"  features shape: {features.shape}, contiguous: {features.is_contiguous()}\")\n",
    "             print(f\"  radial_weights_reshaped shape: {radial_weights_reshaped.shape}, contiguous: {radial_weights_reshaped.is_contiguous()}\")\n",
    "             print(f\"  basis_reshaped shape: {basis_reshaped.shape}, contiguous: {basis_reshaped.is_contiguous()}\")\n",
    "             raise e\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        # k = l = 0 non-fused case (scalar -> scalar)\n",
    "        # features shape (E, D_in=1, C_in)\n",
    "        expected_radial_numel = num_edges * self.channels_out * channels_in_feat * 1 # basis_size=1\n",
    "        if radial_output.numel() != expected_radial_numel:\n",
    "            pass #pass FOR NOW\n",
    "            # raise ValueError(\n",
    "            #     f\"RadialProfile output size mismatch (scalar path)! \"\n",
    "            #     f\"Expected {expected_radial_numel}, got {radial_output.numel()}. \"\n",
    "            #     f\"Shape: {radial_output.shape}. C_out={self.channels_out}, C_in={channels_in_feat}\"\n",
    "            #  )\n",
    "\n",
    "        try:\n",
    "            radial_weights = radial_output.view(num_edges, self.channels_out, channels_in_feat).contiguous() # Line 206 + contiguous\n",
    "        except RuntimeError as e:\n",
    "            print(\"ERROR during .view() (scalar path)!\")\n",
    "            print(f\"  radial_output shape: {radial_output.shape}\")\n",
    "            print(f\"  Target shape: ({num_edges}, {self.channels_out}, {channels_in_feat})\")\n",
    "            raise e\n",
    "\n",
    "        features = features.contiguous()\n",
    "\n",
    "        try:\n",
    "            # Use channels_in_feat from features for einsum consistency\n",
    "            # features shape (E, 1, C_in) -> nli\n",
    "            # radial_weights shape (E, C_out, C_in) -> noi\n",
    "            result = torch.einsum('nli,noi->no', features, radial_weights).unsqueeze(-1) # Line 210. Output (E, C_out, 1)\n",
    "        except RuntimeError as e:\n",
    "             print(\"ERROR during einsum (scalar path)!\")\n",
    "             print(f\"  features shape: {features.shape}, contiguous: {features.is_contiguous()}\")\n",
    "             print(f\"  radial_weights shape: {radial_weights.shape}, contiguous: {radial_weights.is_contiguous()}\")\n",
    "             raise e\n",
    "\n",
    "        return result\n",
    "\n",
    "# 2. Apply the patch\n",
    "VersatileConvSE3.forward = fixed_versatile_forward\n",
    "print(\"Successfully patched VersatileConvSE3.forward.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T04:20:39.425359Z",
     "iopub.status.busy": "2025-05-02T04:20:39.425131Z",
     "iopub.status.idle": "2025-05-02T04:20:39.430499Z",
     "shell.execute_reply": "2025-05-02T04:20:39.430110Z",
     "shell.execute_reply.started": "2025-05-02T04:20:39.425342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched RP\n"
     ]
    }
   ],
   "source": [
    "from se3_transformer.model.layers.convolution import RadialProfile\n",
    "\n",
    "def fixed__init__(\n",
    "            self,\n",
    "            num_freq: int,\n",
    "            channels_in: int,\n",
    "            channels_out: int,\n",
    "            edge_dim: int = 1,\n",
    "            mid_dim: int = 32,\n",
    "            use_layer_norm: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param num_freq:         Number of frequencies\n",
    "        :param channels_in:      Number of input channels\n",
    "        :param channels_out:     Number of output channels\n",
    "        :param edge_dim:         Number of invariant edge features (input to the radial function)\n",
    "        :param mid_dim:          Size of the hidden MLP layers\n",
    "        :param use_layer_norm:   Apply layer normalization between MLP layers\n",
    "        \"\"\"\n",
    "        super(RadialProfile, self).__init__()\n",
    "        modules = [\n",
    "            nn.Linear(edge_dim, mid_dim),\n",
    "            nn.LayerNorm(mid_dim) if use_layer_norm else None,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_dim, mid_dim),\n",
    "            nn.LayerNorm(mid_dim) if use_layer_norm else None,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_dim, num_freq * channels_in * channels_out, bias=False)\n",
    "        ]\n",
    "        self.expected_output_size = num_freq * channels_in * channels_out\n",
    "        self._edge_dim = edge_dim\n",
    "        self._mid_dim = mid_dim\n",
    "        self.net = torch.jit.script(nn.Sequential(*[m for m in modules if m is not None]))\n",
    "\n",
    "RadialProfile.__init__ = fixed__init__\n",
    "\n",
    "print(\"Patched RP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T04:20:41.659784Z",
     "iopub.status.busy": "2025-05-02T04:20:41.659251Z",
     "iopub.status.idle": "2025-05-02T04:20:41.663979Z",
     "shell.execute_reply": "2025-05-02T04:20:41.663406Z",
     "shell.execute_reply.started": "2025-05-02T04:20:41.659766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched RadialProfile.forward with debug prints.\n"
     ]
    }
   ],
   "source": [
    "from se3_transformer.model.layers.convolution import RadialProfile # Make sure it's imported\n",
    "\n",
    "# original_radial_forward = RadialProfile.forward # Store original if needed\n",
    "\n",
    "def patched_radial_forward(self, features: Tensor) -> Tensor:\n",
    "    print(f\"--- INSIDE RadialProfile.forward ---\")\n",
    "    print(f\"    Received features shape: {features.shape}, size: {features.nelement()}\")\n",
    "    # You could even print min/max/mean/isnan to check for bad values\n",
    "    # print(f\"    Features has NaN: {torch.isnan(features).any()}\")\n",
    "    # print(f\"    Features min/max: {features.min()}, {features.max()}\")\n",
    "    try:\n",
    "        result = self.net(features)\n",
    "        # assert result.nelement() == self.expected_output_size, f\"SHAPE MISMATCH IN RADFORWARD: o: {result.nelement()}, e: {self.expected_output_size}\\nedge_dim:{self._edge_dim}, mid_dim: {self._mid_dim}\"\n",
    "        print(f\"Outputting shape: {result.shape}.\")\n",
    "        return result\n",
    "    except RuntimeError as e:\n",
    "        print(f\"    !!! ERROR occurred within self.net(features) !!!\")\n",
    "        raise e\n",
    "\n",
    "RadialProfile.forward = patched_radial_forward\n",
    "print(\"Patched RadialProfile.forward with debug prints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:38:37.400673Z",
     "iopub.status.busy": "2025-05-02T02:38:37.400440Z",
     "iopub.status.idle": "2025-05-02T02:39:38.262058Z",
     "shell.execute_reply": "2025-05-02T02:39:38.261389Z",
     "shell.execute_reply.started": "2025-05-02T02:38:37.400655Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.41.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rna-fm\n",
      "  Downloading rna_fm-0.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rna-fm) (1.26.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from rna-fm) (2.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from rna-fm) (4.66.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from rna-fm) (1.3.0)\n",
      "Collecting ptflops (from rna-fm)\n",
      "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->rna-fm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->rna-fm) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->rna-fm) (2023.4)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops->rna-fm) (2.1.1+cu121)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->rna-fm) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->rna-fm) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->rna-fm) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->rna-fm) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops->rna-fm) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops->rna-fm) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops->rna-fm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops->rna-fm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops->rna-fm) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops->rna-fm) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops->rna-fm) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops->rna-fm) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0->ptflops->rna-fm) (1.3.0)\n",
      "Downloading rna_fm-0.2.2-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: ptflops, rna-fm\n",
      "Successfully installed ptflops-0.7.4 rna-fm-0.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.9.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2023.6.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.66.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (2020.6.20)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///notebooks/SE3Transformer\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: se3-transformer\n",
      "  Running setup.py develop for se3-transformer\n",
      "Successfully installed se3-transformer-1.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting dllogger (from -r requirements.txt (line 4))\n",
      "  Cloning https://github.com/NVIDIA/dllogger to /tmp/pip-install-foqkbxyc/dllogger_c70a1b1ca75e403397348cc7b113f2be\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/dllogger /tmp/pip-install-foqkbxyc/dllogger_c70a1b1ca75e403397348cc7b113f2be\n",
      "  Resolved https://github.com/NVIDIA/dllogger to commit 0478734ff7be75adde8d160e04872664d1c62e5f\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting e3nn==0.3.3 (from -r requirements.txt (line 1))\n",
      "  Downloading e3nn-0.3.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting wandb==0.12.0 (from -r requirements.txt (line 2))\n",
      "  Downloading wandb-0.12.0-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pynvml==11.0.0 (from -r requirements.txt (line 3))\n",
      "  Downloading pynvml-11.0.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from e3nn==0.3.3->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from e3nn==0.3.3->-r requirements.txt (line 1)) (1.11.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from e3nn==0.3.3->-r requirements.txt (line 1)) (2.1.1+cu121)\n",
      "Collecting opt-einsum-fx (from e3nn==0.3.3->-r requirements.txt (line 1))\n",
      "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (3.1.41)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Collecting promise<3,>=2.0 (from wandb==0.12.0->-r requirements.txt (line 2))\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0 (from wandb==0.12.0->-r requirements.txt (line 2))\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (1.39.2)\n",
      "Collecting subprocess32>=3.5.3 (from wandb==0.12.0->-r requirements.txt (line 2))\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (0.4.0)\n",
      "Collecting configparser>=3.8.1 (from wandb==0.12.0->-r requirements.txt (line 2))\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (4.23.4)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.11/dist-packages (from wandb==0.12.0->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=1.0.0->wandb==0.12.0->-r requirements.txt (line 2)) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.12.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb==0.12.0->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.12.0->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb==0.12.0->-r requirements.txt (line 2)) (2020.6.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn==0.3.3->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn==0.3.3->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn==0.3.3->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn==0.3.3->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn==0.3.3->-r requirements.txt (line 1)) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn==0.3.3->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from opt-einsum-fx->e3nn==0.3.3->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from opt-einsum-fx->e3nn==0.3.3->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from scipy->e3nn==0.3.3->-r requirements.txt (line 1)) (1.26.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->e3nn==0.3.3->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.12.0->-r requirements.txt (line 2)) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->e3nn==0.3.3->-r requirements.txt (line 1)) (2.1.4)\n",
      "Downloading e3nn-0.3.3-py3-none-any.whl (382 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.1/382.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.12.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-11.0.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: dllogger, promise, subprocess32\n",
      "  Building wheel for dllogger (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dllogger: filename=DLLogger-1.1.0-py3-none-any.whl size=5660 sha256=34bb5e946c8ba39cb058cee3a42f26f7e5e4660f9f16ca95476f2010f34ab828\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b2t45c51/wheels/54/6b/b5/5cea1063a579199d73df79c0784ab11e9dfc5007bc9fdd9523\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=27ea513b6cee1338e58d64d75209e57a3f589a834cfe9fc562a54de994db4f7c\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=c11a02ca7fcd0f231c5acfafd039c42e98cc2c45c843e2fef9980a417702cc8a\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/b6/54/4477e68f0e313dd4c43b79e6ed43bdcf12abe7626d6d81b369\n",
      "Successfully built dllogger promise subprocess32\n",
      "Installing collected packages: subprocess32, shortuuid, pynvml, promise, dllogger, configparser, wandb, opt-einsum-fx, e3nn\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.15.10\n",
      "    Uninstalling wandb-0.15.10:\n",
      "      Successfully uninstalled wandb-0.15.10\n",
      "Successfully installed configparser-7.2.0 dllogger-1.1.0 e3nn-0.3.3 opt-einsum-fx-0.1.4 promise-2.3 pynvml-11.0.0 shortuuid-1.0.13 subprocess32-3.5.4 wandb-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
      "Collecting dgl\n",
      "  Downloading https://data.dgl.ai/wheels/cu121/dgl-2.1.0%2Bcu121-cp311-cp311-manylinux1_x86_64.whl (467.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.11.2)\n",
      "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.66.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (5.9.8)\n",
      "Collecting torchdata>=0.5.0 (from dgl)\n",
      "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->dgl) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->dgl) (2020.6.20)\n",
      "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata>=0.5.0->dgl) (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
      "Downloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchdata, dgl\n",
      "Successfully installed dgl-2.1.0+cu121 torchdata-0.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCompleted pip process\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install einops\n",
    "!pip install bitsandbytes\n",
    "!pip install rna-fm # https://github.com/ml4bio/RNA-FM\n",
    "!pip install torch_geometric\n",
    "# !pip install viennarna\n",
    "!pip install networkx\n",
    "os.chdir(\"/notebooks/SE3Transformer\")\n",
    "!pip install -e .\n",
    "!pip install -r requirements.txt\n",
    "# !pip install dgl==1.0.0\n",
    "!pip install --pre dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
    "os.chdir(\"/notebooks/proj\")\n",
    "\n",
    "print(\"Completed pip process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:39:42.825439Z",
     "iopub.status.busy": "2025-05-02T02:39:42.824840Z",
     "iopub.status.idle": "2025-05-02T02:39:42.910912Z",
     "shell.execute_reply": "2025-05-02T02:39:42.910367Z",
     "shell.execute_reply.started": "2025-05-02T02:39:42.825415Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/notebooks/SE3Transformer\")\n",
    "import se3_transformer\n",
    "sys.path.append(\"/notebooks/RNAstructure/exe\")\n",
    "import RNAstructure\n",
    "# sys.path.append(\"/notebooks/GCNfold\")\n",
    "# from GCNfold import models\n",
    "# from nets.gcnfold_net import GCNFoldNet_UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:39:49.344766Z",
     "iopub.status.busy": "2025-05-02T02:39:49.344218Z",
     "iopub.status.idle": "2025-05-02T02:39:52.553924Z",
     "shell.execute_reply": "2025-05-02T02:39:52.553449Z",
     "shell.execute_reply.started": "2025-05-02T02:39:49.344748Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:41:21.074262Z",
     "iopub.status.busy": "2025-05-02T02:41:21.073735Z",
     "iopub.status.idle": "2025-05-02T02:41:24.475977Z",
     "shell.execute_reply": "2025-05-02T02:41:24.475494Z",
     "shell.execute_reply.started": "2025-05-02T02:41:21.074234Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.0) (2.0.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.0) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.0) (2.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchdata==0.7.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchdata==0.7.0) (12.9.41)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchdata==0.7.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchdata==0.7.0) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0->torchdata==0.7.0) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0->torchdata==0.7.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#----- MIGHT HAVE TO  RUN BLOCK TWICE TO RESOLVE TYPERROR && DO NOT CHANGE -----##\n",
    "##################################################################################\n",
    "!pip install torchdata==0.7.0\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import yaml\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import fm\n",
    "from sklearn.manifold import MDS\n",
    "from torch_geometric.data import Data\n",
    "import dgl\n",
    "import torch_geometric\n",
    "# import RNA\n",
    "\n",
    "from se3_transformer.model.transformer import SE3Transformer\n",
    "from se3_transformer.model.fiber import Fiber\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CONFIG & SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:41:37.046574Z",
     "iopub.status.busy": "2025-05-02T02:41:37.045903Z",
     "iopub.status.idle": "2025-05-02T02:41:37.052149Z",
     "shell.execute_reply": "2025-05-02T02:41:37.051585Z",
     "shell.execute_reply.started": "2025-05-02T02:41:37.046553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Set a random seed for Python, NumPy, PyTorch (CPU & GPU) to ensure reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Example configuration (you can load this from a YAML, JSON, etc.)\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"cutoff_date\": \"2020-01-01\",\n",
    "    \"test_cutoff_date\": \"2022-05-01\",\n",
    "    \"max_len\": 1024,\n",
    "    \"batch_size\": 1,\n",
    "    \"model_config_path\": \"ribonanzanet2d-final/configs/pairwise.yaml\",\n",
    "    \"max_len_filter\": 1024,\n",
    "    \"min_len_filter\": 10,\n",
    "    \n",
    "    \"train_sequences_path\": \"data/Competition/train_sequences.csv\",\n",
    "    \"train_labels_path\": \"data/Competition/train_labels.csv\",\n",
    "    \"test_data_path\": \"data/Competition/test_sequences.csv\",\n",
    "    \"combined_train_data_path\": \"data/Combined/total_processed_rna_data.pt\",\n",
    "    \"final_pretrained_weights_path\": \"weights/RibonanzaNet-3D-final.pt\",\n",
    "    \"nonfinal_pretrained_weights_path\": \"weights/RibonanzaNet-3D.pt\",\n",
    "    \"save_weights_name\": \"weights/RibonanzaNet-3D.pt\",\n",
    "    \"save_weights_final\": \"weights/RibonanzaNet-3D-final.pt\",\n",
    "    \"rna_fm_weights\": \"weights/RNA-FM_pretrained.pth\",\n",
    "    \"path_to_GCNFold_weights\": \"weights/model_unet_99.pth\",\n",
    "    \"rna_fm_embedding_dim\": 640 # default 640; DO NOT CHANGE\n",
    "}\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "# import shutil\n",
    "# shutil.copy(\"/root/.cache/torch/hub/checkpoints/RNA-FM_pretrained.pth\", config[\"rna_fm_weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA LOADING & PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:41:38.446837Z",
     "iopub.status.busy": "2025-05-02T02:41:38.446275Z",
     "iopub.status.idle": "2025-05-02T02:41:44.224654Z",
     "shell.execute_reply": "2025-05-02T02:41:44.224255Z",
     "shell.execute_reply.started": "2025-05-02T02:41:38.446819Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting XYZ data: 100%|██████████| 844/844 [00:05<00:00, 153.74it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load CSVs\n",
    "train_sequences = pd.read_csv(config[\"train_sequences_path\"])\n",
    "train_labels = pd.read_csv(config[\"train_labels_path\"])\n",
    "\n",
    "# Create a pdb_id field\n",
    "train_labels[\"pdb_id\"] = train_labels[\"ID\"].apply(\n",
    "    lambda x: x.split(\"_\")[0] + \"_\" + x.split(\"_\")[1]\n",
    ")\n",
    "\n",
    "# Collect xyz data for each sequence\n",
    "all_xyz = []\n",
    "for pdb_id in tqdm(train_sequences[\"target_id\"], desc=\"Collecting XYZ data\"):\n",
    "    df = train_labels[train_labels[\"pdb_id\"] == pdb_id]\n",
    "    xyz = df[[\"x_1\", \"y_1\", \"z_1\"]].to_numpy().astype(\"float32\")\n",
    "    xyz[xyz < -1e17] = float(\"nan\")\n",
    "    all_xyz.append(xyz)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 SECONDARY DATA (BPPMs, initial 3D structs, initial sequence embeddings, etc.) GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:07.011775Z",
     "iopub.status.busy": "2025-05-02T02:42:07.011406Z",
     "iopub.status.idle": "2025-05-02T02:42:11.801708Z",
     "shell.execute_reply": "2025-05-02T02:42:11.801246Z",
     "shell.execute_reply.started": "2025-05-02T02:42:07.011775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing 9 ConvTransformerEncoderLayers\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"ribonanzanet2d-final\")\n",
    "\n",
    "from Network import *\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        self.entries=entries\n",
    "\n",
    "    def print(self):\n",
    "        print(self.entries)\n",
    "\n",
    "def load_config_from_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return Config(**config)\n",
    "\n",
    "class finetuned_RibonanzaNet(RibonanzaNet):\n",
    "    def __init__(self, config):\n",
    "        config.dropout=0.2\n",
    "        super(finetuned_RibonanzaNet, self).__init__(config)\n",
    "        self.use_gradient_checkpoint = False\n",
    "        self.ct_predictor=nn.Linear(64,1)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        _, pairwise_features=self.get_embeddings(src, torch.ones_like(src).long().to(src.device))\n",
    "\n",
    "        pairwise_features=pairwise_features+pairwise_features.permute(0,2,1,3) #symmetrize\n",
    "\n",
    "        output=self.ct_predictor(self.dropout(pairwise_features)) #predict\n",
    "\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "ribonet=finetuned_RibonanzaNet(load_config_from_yaml(\"ribonanzanet2d-final/configs/pairwise.yaml\")).cuda()\n",
    "ribonet.load_state_dict(torch.load(\"weights/RibonanzaNet-SS.pt\",map_location='cpu'))\n",
    "ribonet.eval()\n",
    "\n",
    "rna_fmodel, alphabet = fm.pretrained.rna_fm_t12(config[\"rna_fm_weights\"])\n",
    "rnafm_batch_converter = alphabet.get_batch_converter()\n",
    "rna_fmodel.eval()\n",
    "\n",
    "reverse_map = {\n",
    "    0: \"A\", 1: \"C\", 2: \"G\", 3: \"U\"\n",
    "}\n",
    "\n",
    "def tokens_to_str(tokens):\n",
    "    tokens = tokens.tolist()\n",
    "    seq = \"\"\n",
    "    for token in tokens:\n",
    "        seq+=reverse_map[token]\n",
    "    return seq\n",
    "\n",
    "def init_coords_from_sequence(\n",
    "    seq,\n",
    "    bppm,\n",
    "    contact_d=6.0,\n",
    "    noncontact_d=25.0,\n",
    "    mds_kwargs=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq: RNA sequence str of len L\n",
    "        bppm: pair prob matrix of (L, L)\n",
    "        contact_d: target distance (Å) for predicted base pairs\n",
    "        noncontact_d: target distance (Å) for non-paired nucleotides\n",
    "        mds_kwargs: extra args for sklearn.manifold.MDS\n",
    "\n",
    "    Returns:\n",
    "        coords: tensor of shape (L,3)\n",
    "    \"\"\"\n",
    "\n",
    "    P = bppm\n",
    "    \n",
    "    L = P.shape[0]\n",
    "    \n",
    "    # 2. Build graph & run MWM\n",
    "    G = nx.Graph()\n",
    "    for i in range(L):\n",
    "        for j in range(i+4, L):  # enforce minimum loop length\n",
    "            p = P[i, j]\n",
    "            if p > 0.01:  # skip ultra-low probs\n",
    "                w = torch.log(p / (1 - p + 1e-9))\n",
    "                if w > 0:\n",
    "                    G.add_edge(i, j, weight=w)\n",
    "    match = nx.algorithms.matching.max_weight_matching(\n",
    "        G, maxcardinality=False\n",
    "    )  # O(L³) but usually <0.05 s for L≈400\n",
    "\n",
    "    # 3. Build a target distance matrix\n",
    "    D = np.full((L, L), noncontact_d, dtype=float)\n",
    "    for i, j in match:\n",
    "        D[i, j] = D[j, i] = contact_d\n",
    "    np.fill_diagonal(D, 0.0)\n",
    "\n",
    "    # 4. Run classical MDS to embed into ℝ³\n",
    "    mds_kwargs = mds_kwargs or {}\n",
    "    mds = MDS(\n",
    "        n_components=3,\n",
    "        dissimilarity=\"precomputed\",\n",
    "        n_init=4,\n",
    "        max_iter=300,\n",
    "        **mds_kwargs\n",
    "    )\n",
    "    coords = mds.fit_transform(D)  # (L,3), preserves the “contact” proximities\n",
    "    return torch.from_numpy(coords).float().cuda()\n",
    "\n",
    "vocab = {\"A\":0, \"C\":1, \"G\":2, \"U\":3}\n",
    "def get_ribonet_bpp(sequence): # tensor of shape (1, L, L)\n",
    "    src = sequence.unsqueeze(0)\n",
    "    return ribonet(src).sigmoid().detach().cpu()\n",
    "    \n",
    "def get_rnaf_seq_encoding(sequence): \n",
    "    # sequence = tokens_to_str(sequence[0]) # CURRENTLY ONLY SUPPORTS BATCH SIZE 1 ### FIX ###\n",
    "     \n",
    "    # Prepare data\n",
    "    data = [\n",
    "        (\"Sequence\", sequence)\n",
    "    ]\n",
    "    _, _, batch_tokens = rnafm_batch_converter(data) # [(id, seq),...] -> batch label, seq, tokens\n",
    "\n",
    "    # Extract embeddings (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = rna_fmodel(batch_tokens, repr_layers=[rna_fmodel.num_layers])\n",
    "    # print(results[\"representations\"])\n",
    "    token_embeddings = results[\"representations\"][rna_fmodel.num_layers].cuda()\n",
    "    token_embeddings = token_embeddings[:, 1:-1, :]\n",
    "    return token_embeddings # (1, seqlen, 640)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:13:43.014961Z",
     "iopub.status.busy": "2025-04-30T13:13:43.014518Z",
     "iopub.status.idle": "2025-04-30T13:13:43.018720Z",
     "shell.execute_reply": "2025-04-30T13:13:43.018182Z",
     "shell.execute_reply.started": "2025-04-30T13:13:43.014942Z"
    }
   },
   "outputs": [],
   "source": [
    "# # I want to create literal arrays of initial_embedding, initial_3d, bppm for each sequence\n",
    "# # print(train_sequences[\"sequence\"].head())\n",
    "\n",
    "# import csv\n",
    "\n",
    "# init_seq_embeddings, initial_3ds, bppms = [], [], []\n",
    "# invalid_indices = []\n",
    "# def generate_support_data():\n",
    "#     \"\"\"\n",
    "#     Generates all support data and saves to respective arrays. Do not run every time.\n",
    "#     \"\"\"\n",
    "#     total = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     for i, sequence in tqdm(enumerate(train_sequences[\"sequence\"])):\n",
    "#         if len(sequence) > 1024: # RNA-FM constraint\n",
    "#             invalid_indices.append(i)\n",
    "#             init_seq_embeddings.append([])\n",
    "#             initial_3ds.append([])\n",
    "#             bppms.append([])\n",
    "#             total+=1\n",
    "#             continue\n",
    "#         emb = get_rnaf_seq_encoding(sequence)\n",
    "#         init_seq_embeddings.append(emb)\n",
    "#         bppm = get_ribonet_bpp(sequence)\n",
    "#         bppms.append(bppm)\n",
    "#         init3ds = init_coords_from_sequence(sequence, bppm)\n",
    "#         initial_3ds.append(init3ds)\n",
    "#         total+=1\n",
    "#     print(f\"Finished processing {i} sequences\")\n",
    "\n",
    "# print(f\"Generating support data for {len(train_sequences['sequence'])} sequences...\")\n",
    "\n",
    "# generate_support_data()\n",
    "\n",
    "# def load_support_data(path):\n",
    "    \n",
    "#     pass\n",
    "\n",
    "# assert not bppms==[], \"Must either call load or save support data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:11.802757Z",
     "iopub.status.busy": "2025-05-02T02:42:11.802522Z",
     "iopub.status.idle": "2025-05-02T02:42:11.814395Z",
     "shell.execute_reply": "2025-05-02T02:42:11.813784Z",
     "shell.execute_reply.started": "2025-05-02T02:42:11.802757Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence in train: 4298\n"
     ]
    }
   ],
   "source": [
    "valid_indices = []\n",
    "max_len_seen = 0\n",
    "\n",
    "for i, xyz in enumerate(all_xyz):\n",
    "    # Track the maximum length\n",
    "    if len(xyz) > max_len_seen:\n",
    "        max_len_seen = len(xyz)\n",
    "\n",
    "    nan_ratio = np.isnan(xyz).mean()\n",
    "    seq_len = len(xyz)\n",
    "    # Keep sequence if it meets criteria\n",
    "    if (nan_ratio <= 0.5) and (config[\"min_len_filter\"] < seq_len <= config[\"max_len_filter\"]):\n",
    "        valid_indices.append(i)\n",
    "\n",
    "print(f\"Longest sequence in train: {max_len_seen}\")\n",
    "\n",
    "# Filter sequences & xyz based on valid_indices\n",
    "train_sequences = train_sequences.loc[valid_indices].reset_index(drop=True)\n",
    "all_xyz = [all_xyz[i] for i in valid_indices]\n",
    "# init_seq_embeddings = [init_seq_embeddings[i] for i in valid_indices]\n",
    "# initial_3ds = [initial_3ds[i] for i in valid_indices]\n",
    "# bppms = [bppms[i] for i in valid_indices]\n",
    "\n",
    "# Prepare final data dictionary\n",
    "data = {\n",
    "    \"sequence\": train_sequences[\"sequence\"].tolist(),\n",
    "    \"temporal_cutoff\": train_sequences[\"temporal_cutoff\"].tolist(),\n",
    "    \"description\": train_sequences[\"description\"].tolist(),\n",
    "    \"all_sequences\": train_sequences[\"all_sequences\"].tolist(),\n",
    "    \"xyz\": all_xyz\n",
    "    # \"base_pair_matrices\": bppms,\n",
    "    # \"3d_inits\": tertiary_inits,\n",
    "    # \"seq_embedding_inits\": seq_emb_inits\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TRAIN / VAL SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:14.945450Z",
     "iopub.status.busy": "2025-05-02T02:42:14.945151Z",
     "iopub.status.idle": "2025-05-02T02:42:14.950080Z",
     "shell.execute_reply": "2025-05-02T02:42:14.949715Z",
     "shell.execute_reply.started": "2025-05-02T02:42:14.945450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "cutoff_date = pd.Timestamp(config[\"cutoff_date\"])\n",
    "test_cutoff_date = pd.Timestamp(config[\"test_cutoff_date\"])\n",
    "\n",
    "train_indices = [i for i, date_str in enumerate(data[\"temporal_cutoff\"]) if pd.Timestamp(date_str) <= cutoff_date]\n",
    "test_indices = [i for i, date_str in enumerate(data[\"temporal_cutoff\"]) if cutoff_date < pd.Timestamp(date_str) <= test_cutoff_date]\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "all_indices = list(range(len(data[\"sequence\"])))\n",
    "train_indices, test_indices = train_test_split(all_indices, test_size=0.1, random_state=config[\"seed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:16.281870Z",
     "iopub.status.busy": "2025-05-02T02:42:16.281522Z",
     "iopub.status.idle": "2025-05-02T02:42:16.291140Z",
     "shell.execute_reply": "2025-05-02T02:42:16.290602Z",
     "shell.execute_reply.started": "2025-05-02T02:42:16.281851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rna_collate_fn(batch):\n",
    "    sequences = [item[\"sequence\"] for item in batch]\n",
    "    xyzs = [item[\"xyz\"] for item in batch]\n",
    "\n",
    "    # Create masks before padding\n",
    "    masks = [torch.ones(len(seq), dtype=torch.bool) for seq in sequences]\n",
    "\n",
    "    # Pad sequences and coordinates\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=4)  # 4 = <PAD> token index\n",
    "    padded_xyzs = pad_sequence(xyzs, batch_first=True, padding_value=0.0)\n",
    "    padded_masks = pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        \"sequence\": padded_sequences,\n",
    "        \"xyz\": padded_xyzs,\n",
    "        \"mask\": padded_masks\n",
    "    }\n",
    "\n",
    "\n",
    "class RNA3D_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for 3D RNA structures.\n",
    "    \"\"\"\n",
    "    def __init__(self, indices, data_dict, max_len=384):\n",
    "        self.indices = indices\n",
    "        self.data = data_dict\n",
    "        self.max_len = max_len\n",
    "        self.nt_to_idx = {nt: i for i, nt in enumerate(\"ACGU\")}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "   \n",
    "    def clean_sequences(self):\n",
    "        clean_seqs = []\n",
    "        clean_xyz = []\n",
    "        clean_indices = []\n",
    "\n",
    "        for seq, coords in zip(self.data[\"sequence\"], self.data[\"xyz\"]):\n",
    "            if 'X' in seq or coords is None or len(seq) != len(coords):\n",
    "                continue\n",
    "            clean_seqs.append(seq)\n",
    "            clean_xyz.append(coords)\n",
    "\n",
    "        self.data[\"sequence\"] = clean_seqs\n",
    "        self.data[\"xyz\"] = clean_xyz\n",
    "        self.indices = list(range(len(clean_seqs)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_idx = self.indices[idx]\n",
    "        # Convert nucleotides to integer tokens\n",
    "        sequence = []\n",
    "\n",
    "        sequence = [self.nt_to_idx[nt] for nt in self.data[\"sequence\"][data_idx]]\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long)\n",
    "        # Convert xyz to torch tensor\n",
    "        xyz = torch.tensor(self.data[\"xyz\"][data_idx], dtype=torch.float32)\n",
    "\n",
    "        # If sequence is longer than max_len, randomly crop\n",
    "        if len(sequence) > self.max_len:\n",
    "            crop_start = np.random.randint(len(sequence) - self.max_len)\n",
    "            crop_end = crop_start + self.max_len\n",
    "            sequence = sequence[crop_start:crop_end]\n",
    "            xyz = xyz[crop_start:crop_end]\n",
    "\n",
    "        return {\"sequence\": sequence, \"xyz\": xyz}\n",
    "\n",
    "train_dataset = RNA3D_Dataset(train_indices, data, max_len=config[\"max_len\"])\n",
    "train_dataset.clean_sequences()\n",
    "val_dataset = RNA3D_Dataset(test_indices, data, max_len=config[\"max_len\"])\n",
    "val_dataset.clean_sequences()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "    num_workers=8,  # Adjust based on CPU cores\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    collate_fn=rna_collate_fn\n",
    "    )\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=8, pin_memory=True, \n",
    "                        collate_fn=rna_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MODEL, CONFIG CLASSES & HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T02:54:51.320511Z",
     "iopub.status.busy": "2025-05-01T02:54:51.320215Z",
     "iopub.status.idle": "2025-05-01T02:54:51.323518Z",
     "shell.execute_reply": "2025-05-01T02:54:51.322884Z",
     "shell.execute_reply.started": "2025-05-01T02:54:51.320445Z"
    }
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     L = 20\n",
    "#     dummy_S   = torch.randn(L, 640, device='cuda')\n",
    "#     dummy_P   = torch.rand  (L, L,  device='cuda')\n",
    "#     dummy_xyz = torch.randn(L, 3,   device='cuda')\n",
    "#     g = _make_graph(dummy_S, dummy_P, dummy_xyz,\n",
    "#                     model.rbf_mu, model.rbf_sigma, thresh=0.2)\n",
    "#     print(\"scalar width =\", g.edge_feats[0].shape[1])   # must be 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:19.420068Z",
     "iopub.status.busy": "2025-05-02T02:42:19.419696Z",
     "iopub.status.idle": "2025-05-02T02:42:19.436849Z",
     "shell.execute_reply": "2025-05-02T02:42:19.436358Z",
     "shell.execute_reply.started": "2025-05-02T02:42:19.420042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "class SeqPairBlock(nn.Module):\n",
    "    def __init__(self, seq_dim, pair_dim, n_heads=8, use_triangular_attention=True):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(seq_dim, 3*seq_dim)\n",
    "        self.p_bias = nn.Linear(pair_dim, n_heads)           # per‑head scalar bias\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=256, num_heads=8, batch_first=True)\n",
    "        self.triangle_update_out=TriangleMultiplicativeModule(dim=pair_dimension,mix='outgoing')\n",
    "        self.triangle_update_in=TriangleMultiplicativeModule(dim=pair_dimension,mix='ingoing')\n",
    "\n",
    "        self.pair_dropout_out=DropoutRowwise(dropout)\n",
    "        self.pair_dropout_in=DropoutRowwise(dropout)\n",
    "\n",
    "        self.use_triangular_attention=use_triangular_attention\n",
    "\n",
    "        if self.use_triangular_attention:\n",
    "            self.triangle_attention_out=TriangleAttention(in_dim=pair_dimension,\n",
    "                                                                    dim=pair_dimension//4,\n",
    "                                                                    wise='row')\n",
    "            self.triangle_attention_in=TriangleAttention(in_dim=pair_dimension,\n",
    "                                                                    dim=pair_dimension//4,\n",
    "                                                                    wise='col')\n",
    "            self.pair_attention_dropout_out=DropoutRowwise(dropout)\n",
    "            self.pair_attention_dropout_in=DropoutColumnwise(dropout)\n",
    "\n",
    "        self.ffn_seq = nn.Sequential(nn.Linear(seq_dim,4*seq_dim),\n",
    "                                     nn.GELU(),\n",
    "                                     nn.Linear(4*seq_dim,seq_dim))\n",
    "        self.outer_proj = nn.Sequential(nn.Linear(seq_dim*2+seq_dim**2, pair_dim),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(pair_dim,pair_dim))\n",
    "        # self.tri_mult = TriangleMul(d_p)                # optional\n",
    "        # self.tri_att  = TriangleAtt(d_p//2)             # optional\n",
    "\n",
    "    def forward(self, S, P):\n",
    "        # 1. self‑att with pair bias\n",
    "        q,k,v = self.qkv(S).chunk(3,dim=-1)\n",
    "        bias  = self.p_bias(P).permute(2,0,1)           # (heads,L,L)\n",
    "        S = S + self.attn(q,k,v, attn_bias=bias)\n",
    "\n",
    "        # 2. FF on S\n",
    "        S = S + self.ffn_seq(S)\n",
    "\n",
    "        # 3. pair update\n",
    "        op = torch.einsum('id,jd->ijd', S, S)           # outer product\n",
    "        feats = torch.cat((op, S[:,None]+S[None,:]), dim=-1)\n",
    "        P = P + self.outer_proj(feats)\n",
    "\n",
    "        # # 4. triangle refinement every k blocks\n",
    "        # if do_triangle:\n",
    "        #     P = P + self.tri_mult(P)\n",
    "        #     P = P + self.tri_att(P)\n",
    "\n",
    "        return S, P\n",
    "'''\n",
    "\n",
    "class PairEmbedding(nn.Module):\n",
    "    def __init__(self, d_seq, d_pair, d_hidden=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, d_pair)\n",
    "        )\n",
    "        self.outer_product_mean = Outer_Product_Mean(in_dim=d_seq, pairwise_dim=d_pair)\n",
    "        self.rel_pos_embed = relpos(dim=d_pair)\n",
    "\n",
    "    def forward(self, seq_rep, bppm):\n",
    "        print(f\"embedding seq_rep of shape {seq_rep.shape}, and bppm of shape {bppm.shape}\")\n",
    "        x = bppm.unsqueeze(-1)                       # (L,L,1) bppm is len 28\n",
    "        pair_embed = self.mlp(x)                        # (L,L,d_pair)\n",
    "        outer_prod_mean = self.outer_product_mean(seq_rep)  # seq_rep is len 30\n",
    "        rel_embeddings = self.rel_pos_embed(seq_rep)\n",
    "        print(f\"Pair: {pair_embed.shape}, outer: {outer_prod_mean.shape}, relpos: {rel_embeddings.shape}\")\n",
    "        summed_pair_rep = outer_prod_mean + rel_embeddings + pair_embed\n",
    "        return summed_pair_rep\n",
    "\n",
    "class ConvFormerBlocks(nn.Module):\n",
    "    def __init__(self, n_blocks, seq_dim, nhead, pair_dim,\n",
    "                 use_triangular_attention, dropout):\n",
    "        super(ConvFormerBlocks, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConvTransformerEncoderLayer(\n",
    "                d_model = seq_dim,\n",
    "                nhead = nhead,\n",
    "                dim_feedforward = seq_dim*3, \n",
    "                pairwise_dimension= pair_dim,\n",
    "                use_triangular_attention=use_triangular_attention,\n",
    "                dropout = dropout\n",
    "            )\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, seq_embedding, pair_embedding):\n",
    "        print(f\"s: {seq_embedding.shape}, p: {pair_embedding.shape}\")\n",
    "        seqrep = seq_embedding\n",
    "        pairrep = pair_embedding\n",
    "        mask = torch.ones(seqrep.size(0), seqrep.size(1), dtype=torch.bool, device=seqrep.device)\n",
    "        for block in self.blocks:\n",
    "            seqrep, pairrep = block(seqrep, pairrep, src_mask=mask)\n",
    "        return seqrep, pairrep\n",
    "\n",
    "''' PER CLAUDE\n",
    "class SE3FormerBlocks(nn.Module):\n",
    "    def __init__(self, n_blocks, seq_dim, thresh):\n",
    "        super(SE3FormerBlocks, self).__init__()\n",
    "        self.thresh = thresh\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SE3Transformer(\n",
    "                num_layers     = 4,                    # == 4 equivariant blocks\n",
    "                num_heads      = 8,                    # matches DeepMind default\n",
    "                channels_div   = 2,                    # head dim = hidden/2\n",
    "                fiber_in       = Fiber({0: seq_dim, 1: 1}),\n",
    "                fiber_hidden   = Fiber({0:128, 1:128, 2:64}),\n",
    "                fiber_out      = Fiber({1:1}),         # emit coordinate delta\n",
    "                fiber_edge=Fiber({0:32, 1:1}),\n",
    "                edge_dim=32,\n",
    "                use_layer_norm = True,\n",
    "                self_interaction = True,               # linear on each fibre\n",
    "                dropout        = 0.1\n",
    "            )\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "'''\n",
    "\n",
    "class SE3FormerBlocks(nn.Module):\n",
    "    def __init__(self, n_blocks, seq_dim, thresh):\n",
    "        super(SE3FormerBlocks, self).__init__()\n",
    "        self.thresh = thresh\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SE3Transformer(\n",
    "                num_layers     = 4,\n",
    "                num_heads      = 8,\n",
    "                channels_div   = 2,\n",
    "                fiber_in       = Fiber({0: seq_dim, 1: 1}), # Input node fibers\n",
    "                fiber_hidden   = Fiber({0:128, 1:128, 2:64}),\n",
    "                fiber_out      = Fiber({1:1}),         # Output update vector\n",
    "                fiber_edge     = Fiber({0:32, 1:1}), # Edge features expected\n",
    "                edge_dim       = 33, # Expect 32 scalar + 1 norm = 33 total scalar edge dim input for RadialProfile\n",
    "                use_layer_norm = True,\n",
    "                self_interaction = True,\n",
    "                dropout        = 0.1\n",
    "            )\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, seq_rep, bppm, xyz_init, rbf_mu, rbf_sigma, thresh):\n",
    "        xyz = xyz_init\n",
    "        # Ensure seq_rep is (L, D) if it comes in as (1, L, D)\n",
    "        if seq_rep.dim() == 3 and seq_rep.size(0) == 1:\n",
    "             seq_rep = seq_rep.squeeze(0)\n",
    "\n",
    "        for block_idx, block in enumerate(self.blocks):\n",
    "            # Ensure xyz is (L, 3)\n",
    "            if xyz.dim() == 3 and xyz.size(0) == 1:\n",
    "                 xyz = xyz.squeeze(0)\n",
    "            elif xyz.dim() != 2 or xyz.size(1) != 3:\n",
    "                 raise ValueError(f\"xyz shape entering block {block_idx} is wrong: {xyz.shape}\")\n",
    "\n",
    "            # 1. Create PyG Data object containing raw features\n",
    "            data = _make_graph(seq_rep, bppm, xyz, rbf_mu, rbf_sigma, thresh)\n",
    "\n",
    "            # 2. Create DGL graph and make bidirected\n",
    "            src, dst = data.edge_index\n",
    "            num_nodes = xyz.size(0)\n",
    "            # --- FIX: Create graph on CPU ---\n",
    "            g_cpu = dgl.graph((src.cpu(), dst.cpu()), num_nodes=num_nodes)\n",
    "            # --- Convert to bidirected ON CPU ---\n",
    "            g_bi_cpu = dgl.to_bidirected(g_cpu)\n",
    "            # --- Move the bidirected graph to GPU ---\n",
    "            g = g_bi_cpu.to(device)\n",
    "            \n",
    "            # Handle case of graph with no edges after bidirectionality\n",
    "            if g.num_edges() == 0:\n",
    "                print(f\"Warning: Skipping SE3 block {block_idx} due to 0 edges in DGL graph.\")\n",
    "                continue # Skip to the next block or potentially return xyz if it's the last block\n",
    "\n",
    "            # 3. Prepare Edge Features for DGL graph (duplicate for bidirected)\n",
    "            edge_feats_dgl = {}\n",
    "            for k, v in data.edge_feats.items(): # v is (E_orig, Dim, Channels)\n",
    "                edge_feats_dgl[k] = torch.cat([v, v], dim=0) # Shape (E_dgl, Dim, Channels)\n",
    "\n",
    "            # 4. Calculate Relative Positions for DGL graph\n",
    "            u, v_ = g.edges() # Source and destination indices for DGL edges (E_dgl,)\n",
    "            rel_pos = xyz[v_] - xyz[u] # Shape (E_dgl, 3)\n",
    "            g.edata['rel_pos'] = rel_pos\n",
    "\n",
    "            # 5. Prepare Node Features for DGL message passing context\n",
    "            #    Fetch features corresponding to *source* nodes (u) and reshape/transpose\n",
    "            node_feats_dgl = {}\n",
    "            # Degree 0: Expected shape (E_dgl, 1, 640)\n",
    "            node_feats_0_orig = data.node_feats['0'][u] # Shape (E_dgl, 640, 1)\n",
    "            node_feats_dgl['0'] = node_feats_0_orig.transpose(1, 2) # Shape (E_dgl, 1, 640)\n",
    "\n",
    "            # Degree 1: Expected shape (E_dgl, 3, 1)\n",
    "            node_feats_1_orig = data.node_feats['1'][u] # Shape (E_dgl, 3, 1)\n",
    "            node_feats_dgl['1'] = node_feats_1_orig # Already in correct format\n",
    "            print(\"PRINTING KEYS::::--------\")\n",
    "            print(f\"nodefeats keys: {list(node_feats_dgl.keys())}, edgefeats keys: {list(edge_feats_dgl.keys())}\")\n",
    "            # # --- Assertions for prepared DGL features ---\n",
    "            # first_layer_fiber_in = self.blocks[block_idx].graph_modules[0].fiber_in\n",
    "            # assert node_feats_dgl[0].shape == (g.num_edges(), 1, first_layer_fiber_in[0])\n",
    "            # assert node_feats_dgl[1].shape == (g.num_edges(), 3, first_layer_fiber_in[1])\n",
    "            # assert edge_feats_dgl[0].shape == (g.num_edges(), 32, 1), f\"expected shape {(g.num_edges(), 33, 1)} but got {edge_feats_dgl[0].shape}\"\n",
    "            # assert edge_feats_dgl[1].shape == (g.num_edges(), 3, 1)\n",
    "\n",
    "            # --- End Assertions ---\n",
    "\n",
    "            print(f\"  Verifying shapes before block call...\")\n",
    "            first_layer_fiber_in = self.blocks[block_idx].graph_modules[0].fiber_in\n",
    "            assert node_feats_dgl['0'].shape == (g.num_edges(), 1, first_layer_fiber_in[0]), f\"Node feat 0 shape mismatch: {node_feats_dgl['0'].shape}\"\n",
    "            assert node_feats_dgl['1'].shape == (g.num_edges(), 3, first_layer_fiber_in[1]), f\"Node feat 1 shape mismatch: {node_feats_dgl['1'].shape}\"\n",
    "            assert edge_feats_dgl['0'].shape == (g.num_edges(), 32, 1), f\"Edge feat 0 shape mismatch: {edge_feats_dgl['0'].shape}\" # Expect 32 now\n",
    "            assert edge_feats_dgl['1'].shape == (g.num_edges(), 3, 1), f\"Edge feat 1 shape mismatch: {edge_feats_dgl['1'].shape}\"\n",
    "            print(f\"     Shapes verified.\")\n",
    "            \n",
    "            # 6. Call the SE3Transformer block\n",
    "            out_feats = block(\n",
    "                g,\n",
    "                node_feats_dgl, # Pass features indexed by source nodes & correctly shaped\n",
    "                edge_feats=edge_feats_dgl\n",
    "            )\n",
    "\n",
    "            # 7. Process Output\n",
    "            if 1 not in out_feats:\n",
    "                 raise KeyError(f\"SE3Transformer block {block_idx} output missing type 1 features.\")\n",
    "\n",
    "            # Output features are aggregated at destination nodes, shape (L, Dim, Channels)\n",
    "            xyz_change = out_feats[1].squeeze(-1) # Shape (L, 3)\n",
    "\n",
    "            if xyz.shape != xyz_change.shape:\n",
    "                 raise ValueError(f\"Shape mismatch after block {block_idx}: xyz ({xyz.shape}) vs xyz_change ({xyz_change.shape})\")\n",
    "\n",
    "        xyz = xyz + xyz_change\n",
    "\n",
    "        return xyz\n",
    "\n",
    "''' PER CLAUDE\n",
    "    def forward(self, seq_rep, bppm, xyz_init, rbf_mu, rbf_sigma, thresh):\n",
    "        xyz = xyz_init\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            data = _make_graph(seq_rep, bppm, xyz, rbf_mu, rbf_sigma, thresh)\n",
    "            \n",
    "            edge_feats = data.edge_feats[0].squeeze(-1)  # (E, D)\n",
    "            print(\"edge_feats.shape:\", edge_feats.shape)\n",
    "            # → should be (98, 33) --> correct\n",
    "\n",
    "            src, dst = data.edge_index   # each is a 1-D tensor of length E\n",
    "            # num_nodes = data.node_feats[0].shape[0]        # L\n",
    "            g = dgl.graph((src.tolist(), dst.tolist()))\n",
    "            g = dgl.to_bidirected(g).to(device)\n",
    "\n",
    "  \n",
    "            for k,v in data.edge_feats.items():\n",
    "                # v was (E, F_k), but g has 2E edges now\n",
    "                # data.edge_feats[k] = torch.cat([v, v], dim=0)  # now (2E, F_k)\n",
    "                data.edge_feats[k] = torch.cat([v, v], dim=0)\n",
    "            \n",
    "            u, v_ = g.edges()                      # each is (2E,) LongTensor\n",
    "            rel_pos = xyz[v_] - xyz[u]             # (2E, 3)\n",
    "            # 3) stash it in g.edata\n",
    "            g.edata['rel_pos'] = rel_pos\n",
    "            \n",
    "            node_feats = {\n",
    "                \"0\": data.node_feats[0],     \n",
    "                \"1\": data.node_feats[1]      \n",
    "            }\n",
    "            edge_feats = {\n",
    "                \"0\": data.edge_feats[0],      # (E, #scalar_feats)\n",
    "                \"1\": data.edge_feats[1]       # (E, 3, 1)\n",
    "            }\n",
    "            assert set(node_feats.keys()) == {\"0\", \"1\"},      \"node_feats must have exactly keys 0 and 1\"\n",
    "            assert node_feats[\"0\"].shape[1] == 640,       f\"expected {640} scalars, got {node_feats[0].shape[1]}\"\n",
    "            assert node_feats[\"1\"].shape[1:] == (3, 1),      \"ℓ=1 features must be of shape (L,3,1)\"\n",
    "            # assert set(edge_feats.keys()) == {\"0\", \"1\"},      \"edge_feats must have exactly keys 0 and 1\"\n",
    "            # assert edge_feats[\"1\"].shape[2] == 1,           \"you must provide exactly one vector channel\"\n",
    "            out_feats = block(\n",
    "                g,\n",
    "                node_feats,\n",
    "                edge_feats=edge_feats\n",
    "            )\n",
    "\n",
    "            xyz_change = out_feats[1].squeeze(-1)\n",
    "\n",
    "            xyz = xyz + xyz_change\n",
    "        return xyz\n",
    "'''\n",
    "print(\"Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL INSTANTANTIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:24.597473Z",
     "iopub.status.busy": "2025-05-02T02:42:24.597117Z",
     "iopub.status.idle": "2025-05-02T02:42:25.257436Z",
     "shell.execute_reply": "2025-05-02T02:42:25.257054Z",
     "shell.execute_reply.started": "2025-05-02T02:42:24.597454Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing fresh model...\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=2, chan_out=32\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=2, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=2, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=640, chan_out=32\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=640, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=640, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=64\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=128\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=64, chan_out=1\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=128, chan_out=1\n",
      "DEBUG VersatileConvSE3 Init: edge_dim=33\n",
      "  chan_in=129, chan_out=1\n",
      "insted\n"
     ]
    }
   ],
   "source": [
    "# SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks\n",
    "\n",
    "def _make_graph(S, P, xyz, rbf_mu, rbf_sigma, thresh=0.2):\n",
    "    \"\"\"\n",
    "    S  : (L, d_seq)   – updated sequence scalars\n",
    "    P  : (L, L)       – pair probabilities\n",
    "    xyz: (L, 3)       – C1′ coordinates from RNAComposer/FARNA\n",
    "    thresh: float     - threshold for high prob contact classification\n",
    "    returns PyG Data with edge scalars & vectors ready for SE3-Trf.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(xyz, np.ndarray):\n",
    "        xyz = torch.from_numpy(xyz).cuda()\n",
    "    print(xyz.shape)\n",
    "    # 2) Squeeze off a leading batch dim if present\n",
    "    #    Now xyz should be exactly (L,3)\n",
    "    if xyz.dim() == 3 and xyz.size(0) == 1:\n",
    "        xyz = xyz.squeeze(0)\n",
    "    elif xyz.dim() == 1 and xyz.numel() == 3:\n",
    "        raise ValueError(\"xyz looks like a single point; did you pass the wrong tensor?\")\n",
    "    if S.dim() == 3 and S.size(0) == 1:\n",
    "        S = S.squeeze(0)       # now (L, d_seq)\n",
    "    if P.dim() == 3 and P.size(0) == 1:\n",
    "        P = P.squeeze(0)       # now (L, L)\n",
    "\n",
    "    L = xyz.size(0)\n",
    "    src, dst, e_scalar, e_vec = [], [], [], []\n",
    "\n",
    "    def _add_edge(i, j, etype, pij):\n",
    "        src.append(i); dst.append(j)\n",
    "\n",
    "        rel_pos = xyz[j] - xyz[i] # Shape (3,)\n",
    "        d = torch.norm(rel_pos)\n",
    "\n",
    "        rbf_feat = rbf(d, rbf_mu, rbf_sigma)  # (30)\n",
    "        # base_feats = torch.tensor([etype, pij, d], device=P.device)\n",
    "        # e_scalar.append(torch.cat([base_feats, rbf_feat], dim=0))  # (33,)\n",
    "        e_scalar.append(torch.cat([torch.tensor([etype, pij], device=P.device),rbf_feat], dim=0))\n",
    "        \n",
    "        norm = d + 1e-6 # Add epsilon for stability\n",
    "        e_vec.append(rel_pos / norm)\n",
    "\n",
    "    # (a) backbone\n",
    "    for i in range(L - 1):\n",
    "        _add_edge(i, i + 1, etype=0, pij=1.0)\n",
    "\n",
    "    # (b) high-prob contacts\n",
    "    idx_i, idx_j = torch.where(P > thresh)\n",
    "    for i, j in zip(idx_i.tolist(), idx_j.tolist()):\n",
    "        if j <= i + 2:                 # skip tiny loops\n",
    "            continue\n",
    "        _add_edge(i, j, etype=1, pij=P[i, j].item())\n",
    "\n",
    "    # pack tensors\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long, device=P.device)\n",
    "    e_scalar   = torch.stack(e_scalar, dim=0)                 # (E, 18)\n",
    "    e_vec      = torch.stack(e_vec, dim=0)                    # (E, 3)\n",
    "    \n",
    "    # node features: scalar S_i  (degree-0), vector xyz_i (degree-1)\n",
    "    node_scalars = S                                           # (L, 3, 1)\n",
    "    print(f\"ns: {node_scalars.shape}, nv: {xyz.shape}, es: {e_scalar.shape}, ev: {e_vec.shape}\")\n",
    "    # SE3-Transformer (Fabian Fuchs) expects dicts keyed by degree\n",
    "    node_feats = {'0': node_scalars.unsqueeze(-1), '1': xyz.unsqueeze(-1)}\n",
    "    edge_feats = {'0': e_scalar.unsqueeze(-1), '1': e_vec.unsqueeze(-1)}       # (E, 3, 1)\n",
    "    \n",
    "    print(\"--- INSIDE _make_graph (End) ---\")\n",
    "    print(f\"    Output edge_feats keys: {list(edge_feats.keys())}\")\n",
    "    if 0 in edge_feats:\n",
    "        print(f\"    Output edge_feats[0] shape: {edge_feats[0].shape}\")\n",
    "    else:\n",
    "        print(\"    Output edge_feats has NO key '0'\")\n",
    "    if 1 in edge_feats:\n",
    "        print(f\"    Output edge_feats[1] shape: {edge_feats[1].shape}\")\n",
    "\n",
    "    data = Data()\n",
    "    data.edge_index = edge_index\n",
    "    data.node_feats = node_feats\n",
    "    data.edge_feats = edge_feats\n",
    "    print(f\"  Output data.edge_index shape: {data.edge_index.shape}\")\n",
    "    return data\n",
    "\n",
    "def rbf(d, centers, widths):\n",
    "    \"\"\"Gaussian radial basis for a distance tensor d (..., 1).\"\"\"\n",
    "    return torch.exp(-((d - centers) ** 2) / (2 * widths ** 2))\n",
    "\n",
    "\n",
    "class ChocolateNet(nn.Module):\n",
    "    \"\"\"\n",
    "    pretrained_state: either 0, 1, or 2 depending on how weights should be loaded:\n",
    "    - 0: no pretraining\n",
    "    - 1: load non-final pretrained weights\n",
    "    - 2: load final pretrained weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=0.20, pretrained_state=0, dropout=0.1):\n",
    "\n",
    "        super(ChocolateNet,self).__init__()\n",
    "        if pretrained_state==2:\n",
    "            print(\"loading final pretrained weights...\")\n",
    "            self.load_state_dict(\n",
    "                torch.load(config[\"final_pretrained_weights_path\"], map_location=\"cpu\"), strict = True\n",
    "            )\n",
    "        elif pretrained_state==1:\n",
    "            print(\"loading nonfinal pretrained weights...\")\n",
    "            self.load_state_dict(\n",
    "                torch.load(config[\"nonfinal_pretrained_weights_path\"], map_location=\"cpu\"), strict = True\n",
    "            )\n",
    "        elif pretrained_state==0:\n",
    "            print(\"initializing fresh model...\")\n",
    "        else:\n",
    "            raise ValueError(\"Unknown pretrained_state configuration. See class description.\")\n",
    "        \n",
    "        self.config = {\"gradient_accumulation_steps\": 1}\n",
    "        self.thresh = thresh\n",
    "        self.seq_dim = config[\"rna_fm_embedding_dim\"]\n",
    "        self.pair_dim = 128\n",
    "        self.heads = 8\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.pair_embedding = PairEmbedding(self.seq_dim, self.pair_dim)\n",
    "\n",
    "        self.sequence_transformer = ConvFormerBlocks(\n",
    "            n_blocks = 3,\n",
    "            seq_dim = self.seq_dim, \n",
    "            nhead = self.heads, \n",
    "            pair_dim = self.pair_dim,\n",
    "            use_triangular_attention=True,\n",
    "            dropout = dropout\n",
    "        )\n",
    "        \n",
    "        # (3) RBF parameters for edge-length encoding\n",
    "        mu = torch.linspace(0, 20, 30)               # 30 Gaussians\n",
    "        sigma = 0.8 * torch.ones_like(mu)\n",
    "        self.register_buffer(\"rbf_mu\", mu)\n",
    "        self.register_buffer(\"rbf_sigma\", sigma)\n",
    "        \n",
    "        self.se3_transformer = SE3FormerBlocks(\n",
    "            n_blocks = 1, seq_dim=self.seq_dim, thresh=self.thresh\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        sequence = sequence[0] # DOES NOT SUPPORT BATCH SIZE > 1, FIX!!\n",
    "        print(sequence.shape)\n",
    "        # 1) Get raw RNA-FM embeddings (1, L, d_seq)\n",
    "        fm_emb = get_rnaf_seq_encoding(sequence).cuda()      # → torch.FloatTensor on CPU\n",
    "\n",
    "        # 2) Get BPPM from RiboNet, convert to Tensor\n",
    "        bppm = get_ribonet_bpp(sequence).float().cuda()\n",
    "        # 3) Now build your pair embedding correctly\n",
    "        pair_embedding = self.pair_embedding(fm_emb, bppm)      # both use L\n",
    "        bppm_raw = bppm.squeeze(0)\n",
    "        \n",
    "        # # fm_embedding = get_rnaf_seq_encoding(sequence[0])\n",
    "        # bppm = get_ribonet_bpp(sequence[0])\n",
    "        # bppm_src = torch.from_numpy(bppm).float().cuda()\n",
    "        \n",
    "        \n",
    "        # pair_embedding = self.pair_embedding(fm_embedding, bppm_src)\n",
    "        \n",
    "        xyz_init = init_coords_from_sequence(sequence, bppm_raw)\n",
    "        seq_rep, pair_rep = self.sequence_transformer(fm_emb, pair_embedding)\n",
    "        xyz_pred = self.se3_transformer(\n",
    "                    seq_rep, bppm_raw, xyz_init, self.rbf_mu, self.rbf_sigma, self.thresh\n",
    "                    )\n",
    "        \n",
    "        return xyz_pred\n",
    "        \n",
    "# Instantiate the model\n",
    "model = ChocolateNet().cuda()\n",
    "print(\"insted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T04:00:26.058072Z",
     "iopub.status.busy": "2025-05-02T04:00:26.057467Z",
     "iopub.status.idle": "2025-05-02T04:00:26.065446Z",
     "shell.execute_reply": "2025-05-02T04:00:26.064756Z",
     "shell.execute_reply.started": "2025-05-02T04:00:26.058049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): AttentionBlockSE3(\n",
      "    (to_key_value): ConvSE3(\n",
      "      (conv): ModuleDict(\n",
      "        (1,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (to_query): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 64x640 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 64x1 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "    (attention): AttentionSE3()\n",
      "    (project): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 128x704 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 128x65 (cuda:0)]\n",
      "          (2): Parameter containing: [torch.cuda.FloatTensor of size 64x32 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): NormSE3(\n",
      "    (nonlinearity): ReLU()\n",
      "    (layer_norms): ModuleDict(\n",
      "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (2): AttentionBlockSE3(\n",
      "    (to_key_value): ConvSE3(\n",
      "      (conv): ModuleDict(\n",
      "        (2,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (to_query): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 64x128 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 64x128 (cuda:0)]\n",
      "          (2): Parameter containing: [torch.cuda.FloatTensor of size 32x64 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "    (attention): AttentionSE3()\n",
      "    (project): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 128x192 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 128x192 (cuda:0)]\n",
      "          (2): Parameter containing: [torch.cuda.FloatTensor of size 64x96 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): NormSE3(\n",
      "    (nonlinearity): ReLU()\n",
      "    (layer_norms): ModuleDict(\n",
      "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (4): AttentionBlockSE3(\n",
      "    (to_key_value): ConvSE3(\n",
      "      (conv): ModuleDict(\n",
      "        (2,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (to_query): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 64x128 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 64x128 (cuda:0)]\n",
      "          (2): Parameter containing: [torch.cuda.FloatTensor of size 32x64 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "    (attention): AttentionSE3()\n",
      "    (project): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 128x192 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 128x192 (cuda:0)]\n",
      "          (2): Parameter containing: [torch.cuda.FloatTensor of size 64x96 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): NormSE3(\n",
      "    (nonlinearity): ReLU()\n",
      "    (layer_norms): ModuleDict(\n",
      "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (6): AttentionBlockSE3(\n",
      "    (to_key_value): ConvSE3(\n",
      "      (conv): ModuleDict(\n",
      "        (2,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (0,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,2): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,0): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1,1): VersatileConvSE3(\n",
      "          (radial_func): RadialProfile(\n",
      "            (net): RecursiveScriptModule(\n",
      "              original_name=Sequential\n",
      "              (0): RecursiveScriptModule(original_name=Linear)\n",
      "              (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (2): RecursiveScriptModule(original_name=ReLU)\n",
      "              (3): RecursiveScriptModule(original_name=Linear)\n",
      "              (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "              (5): RecursiveScriptModule(original_name=ReLU)\n",
      "              (6): RecursiveScriptModule(original_name=Linear)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (to_query): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 64x128 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 64x128 (cuda:0)]\n",
      "          (2): Parameter containing: [torch.cuda.FloatTensor of size 32x64 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "    (attention): AttentionSE3()\n",
      "    (project): LinearSE3(\n",
      "      (weights): ParameterDict(\n",
      "          (0): Parameter containing: [torch.cuda.FloatTensor of size 128x192 (cuda:0)]\n",
      "          (1): Parameter containing: [torch.cuda.FloatTensor of size 128x192 (cuda:0)]\n",
      "          (2): Parameter containing: [torch.cuda.FloatTensor of size 64x96 (cuda:0)]\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): NormSE3(\n",
      "    (nonlinearity): ReLU()\n",
      "    (layer_norms): ModuleDict(\n",
      "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (8): ConvSE3(\n",
      "    (conv): ModuleDict(\n",
      "      (2,1): VersatileConvSE3(\n",
      "        (radial_func): RadialProfile(\n",
      "          (net): RecursiveScriptModule(\n",
      "            original_name=Sequential\n",
      "            (0): RecursiveScriptModule(original_name=Linear)\n",
      "            (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "            (2): RecursiveScriptModule(original_name=ReLU)\n",
      "            (3): RecursiveScriptModule(original_name=Linear)\n",
      "            (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "            (5): RecursiveScriptModule(original_name=ReLU)\n",
      "            (6): RecursiveScriptModule(original_name=Linear)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (0,1): VersatileConvSE3(\n",
      "        (radial_func): RadialProfile(\n",
      "          (net): RecursiveScriptModule(\n",
      "            original_name=Sequential\n",
      "            (0): RecursiveScriptModule(original_name=Linear)\n",
      "            (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "            (2): RecursiveScriptModule(original_name=ReLU)\n",
      "            (3): RecursiveScriptModule(original_name=Linear)\n",
      "            (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "            (5): RecursiveScriptModule(original_name=ReLU)\n",
      "            (6): RecursiveScriptModule(original_name=Linear)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1,1): VersatileConvSE3(\n",
      "        (radial_func): RadialProfile(\n",
      "          (net): RecursiveScriptModule(\n",
      "            original_name=Sequential\n",
      "            (0): RecursiveScriptModule(original_name=Linear)\n",
      "            (1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "            (2): RecursiveScriptModule(original_name=ReLU)\n",
      "            (3): RecursiveScriptModule(original_name=Linear)\n",
      "            (4): RecursiveScriptModule(original_name=LayerNorm)\n",
      "            (5): RecursiveScriptModule(original_name=ReLU)\n",
      "            (6): RecursiveScriptModule(original_name=Linear)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (to_kernel_self): ParameterDict(  (1): Parameter containing: [torch.cuda.FloatTensor of size 1x128 (cuda:0)])\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.se3_transformer.blocks[0].graph_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:29.060051Z",
     "iopub.status.busy": "2025-05-02T02:42:29.059686Z",
     "iopub.status.idle": "2025-05-02T02:42:29.068452Z",
     "shell.execute_reply": "2025-05-02T02:42:29.068082Z",
     "shell.execute_reply.started": "2025-05-02T02:42:29.060051Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_distance_matrix(X, Y, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Calculate pairwise distances between every point in X and every point in Y.\n",
    "    Shape: (len(X), len(Y))\n",
    "    \"\"\"\n",
    "    return ((X[:, None] - Y[None, :])**2 + epsilon).sum(dim=-1).sqrt()\n",
    "\n",
    "def dRMSD(pred_x, pred_y, gt_x, gt_y, epsilon=1e-4, Z=10, d_clamp=None):\n",
    "    \"\"\"\n",
    "    Distance-based RMSD.\n",
    "    pred_x, pred_y: predicted coordinates (usually the same tensor for X and Y).\n",
    "    gt_x, gt_y: ground truth coordinates.\n",
    "    \"\"\"\n",
    "    pred_dm = calculate_distance_matrix(pred_x, pred_y)\n",
    "    gt_dm = calculate_distance_matrix(gt_x, gt_y)\n",
    "\n",
    "    mask = ~torch.isnan(gt_dm)\n",
    "    mask[torch.eye(mask.shape[0], device=mask.device).bool()] = False\n",
    "\n",
    "    diff_sq = (pred_dm[mask] - gt_dm[mask])**2 + epsilon\n",
    "    if d_clamp is not None:\n",
    "        diff_sq = diff_sq.clamp(max=d_clamp**2)\n",
    "\n",
    "    return diff_sq.sqrt().mean() / Z\n",
    "\n",
    "def local_dRMSD(pred_x, pred_y, gt_x, gt_y, epsilon=1e-4, Z=10, d_clamp=30):\n",
    "    \"\"\"\n",
    "    Local distance-based RMSD, ignoring distances above a clamp threshold.\n",
    "    \"\"\"\n",
    "    pred_dm = calculate_distance_matrix(pred_x, pred_y)\n",
    "    gt_dm = calculate_distance_matrix(gt_x, gt_y)\n",
    "\n",
    "    mask = (~torch.isnan(gt_dm)) & (gt_dm < d_clamp)\n",
    "    mask[torch.eye(mask.shape[0], device=mask.device).bool()] = False\n",
    "\n",
    "    diff_sq = (pred_dm[mask] - gt_dm[mask])**2 + epsilon\n",
    "    return diff_sq.sqrt().mean() / Z\n",
    "\n",
    "def dRMAE(pred_x, pred_y, gt_x, gt_y, epsilon=1e-4, Z=10):\n",
    "    \"\"\"\n",
    "    Distance-based Mean Absolute Error.\n",
    "    \"\"\"\n",
    "    pred_dm = calculate_distance_matrix(pred_x, pred_y)\n",
    "    gt_dm = calculate_distance_matrix(gt_x, gt_y)\n",
    "\n",
    "    mask = ~torch.isnan(gt_dm)\n",
    "    mask[torch.eye(mask.shape[0], device=mask.device).bool()] = False\n",
    "\n",
    "    diff = torch.abs(pred_dm[mask] - gt_dm[mask])\n",
    "    return diff.mean() / Z\n",
    "\n",
    "def align_svd_mae(input_coords, target_coords, Z=10):\n",
    "    \"\"\"\n",
    "    Align input_coords to target_coords via SVD (Kabsch algorithm) and compute MAE.\n",
    "    \"\"\"\n",
    "    assert input_coords.shape == target_coords.shape, \"Input and target must have the same shape\"\n",
    "\n",
    "    # Create mask for valid points\n",
    "    mask = ~torch.isnan(target_coords.sum(dim=-1))\n",
    "    input_coords = input_coords[mask]\n",
    "    target_coords = target_coords[mask]\n",
    "    \n",
    "    # Compute centroids\n",
    "    centroid_input = input_coords.mean(dim=0, keepdim=True)\n",
    "    centroid_target = target_coords.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Center the points\n",
    "    input_centered = input_coords - centroid_input\n",
    "    target_centered = target_coords - centroid_target\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = input_centered.T @ target_centered\n",
    "\n",
    "    # SVD to find optimal rotation\n",
    "    U, S, Vt = torch.svd(cov_matrix)\n",
    "    R = Vt @ U.T\n",
    "\n",
    "    # Ensure a proper rotation (determinant R == 1)\n",
    "    if torch.det(R) < 0:\n",
    "        Vt_adj = Vt.clone()   # Clone to avoid in-place modification issues\n",
    "        Vt_adj[-1, :] = -Vt_adj[-1, :]\n",
    "        R = Vt_adj @ U.T\n",
    "\n",
    "    # Rotate input and compute mean absolute error\n",
    "    aligned_input = (input_centered @ R.T) + centroid_target\n",
    "    return torch.abs(aligned_input - target_coords).mean() / Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T02:42:30.936127Z",
     "iopub.status.busy": "2025-05-02T02:42:30.935772Z",
     "iopub.status.idle": "2025-05-02T02:42:30.944225Z",
     "shell.execute_reply": "2025-05-02T02:42:30.943828Z",
     "shell.execute_reply.started": "2025-05-02T02:42:30.936107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT TRAIN() FROM SE3TRANSFORMER\n",
    "\n",
    "def train_model(model, train_dl, val_dl, epochs=50, cos_epoch=35, lr=3e-4, clip=1):\n",
    "    \"\"\"Train the model with a CosineAnnealingLR after `cos_epoch` epochs.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.0, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=(epochs - cos_epoch) * len(train_dl),\n",
    "    )\n",
    "    grad_accum_steps = model.config[\"gradient_accumulation_steps\"]\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_preds = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_dl, desc=f\"Training Epoch {epoch+1}/{epochs}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Add profiling for the first few batches of the first epoch\n",
    "        profiling_enabled = (epoch == 0)\n",
    "\n",
    "        for idx, batch in enumerate(train_pbar):\n",
    "\n",
    "            sequence = batch[\"sequence\"].cuda()\n",
    "            \n",
    "            gt_xyz = batch[\"xyz\"].squeeze().cuda()\n",
    "            #mask = batch[\"mask\"].cuda()\n",
    "            # Only profile the first 5 batches of the first epoch\n",
    "            if profiling_enabled and idx < 10:\n",
    "                torch.cuda.synchronize()\n",
    "                start_forward = time.time()\n",
    "                \n",
    "                # Remove autocast\n",
    "                pred_xyz = model(sequence).squeeze()\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                forward_time = time.time() - start_forward\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                start_loss = time.time()\n",
    "                \n",
    "                # Remove autocast\n",
    "                loss = dRMAE(pred_xyz, pred_xyz, gt_xyz, gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                loss_time = time.time() - start_loss\n",
    "                \n",
    "                print(f\"Batch {idx}: Forward pass: {forward_time:.4f}s, Loss computation: {loss_time:.4f}s\")\n",
    "                \n",
    "                # Continue with normal training flow (without scaler)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                # Normal non-profiling training code (without autocast and scaler)\n",
    "                pred_xyz = model(sequence).squeeze()\n",
    "                loss = dRMAE(pred_xyz, pred_xyz, gt_xyz, gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n",
    "            \n",
    "            loss = loss / grad_accum_steps\n",
    "            loss.backward()\n",
    "            if (idx + 1) % grad_accum_steps == 0 or (idx + 1) == len(train_dl):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if (epoch + 1) > cos_epoch:\n",
    "                    scheduler.step()\n",
    "                            \n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (idx + 1)\n",
    "            train_pbar.set_description(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(val_dl):\n",
    "                sequence = batch[\"sequence\"].cuda()\n",
    "                gt_xyz = batch[\"xyz\"].squeeze().cuda()\n",
    "                #mask = batch[\"mask\"].cuda()\n",
    "                pred_xyz = model(sequence).squeeze()\n",
    "                loss = dRMAE(pred_xyz, pred_xyz, gt_xyz, gt_xyz)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_preds.append((gt_xyz.cpu().numpy(), pred_xyz.cpu().numpy()))\n",
    "\n",
    "            val_loss /= len(val_dl)\n",
    "            print(f\"Validation Loss (Epoch {epoch+1}): {val_loss:.4f}\")\n",
    "\n",
    "            # Check for improvement\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_preds = val_preds\n",
    "                torch.save(model.state_dict(), config[\"save_weights_name\"])\n",
    "                print(f\"  -> New best model saved at epoch {epoch+1}\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), config[\"save_weights_final\"])\n",
    "    return best_val_loss, best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. RUN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:37:03.633238Z",
     "iopub.status.busy": "2025-05-01T21:37:03.632511Z",
     "iopub.status.idle": "2025-05-01T21:37:03.635701Z",
     "shell.execute_reply": "2025-05-01T21:37:03.635364Z",
     "shell.execute_reply.started": "2025-05-01T21:37:03.633218Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured batch size: 1\n",
      "Train loader batch size: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Configured batch size: {config['batch_size']}\")\n",
    "print(f\"Train loader batch size: {train_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T04:20:49.586062Z",
     "iopub.status.busy": "2025-05-02T04:20:49.585575Z",
     "iopub.status.idle": "2025-05-02T04:20:51.146039Z",
     "shell.execute_reply": "2025-05-02T04:20:51.145038Z",
     "shell.execute_reply.started": "2025-05-02T04:20:49.586044Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50:   0%|          | 0/731 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75])\n",
      "embedding seq_rep of shape torch.Size([1, 75, 640]), and bppm of shape torch.Size([1, 75, 75])\n",
      "Pair: torch.Size([1, 75, 75, 128]), outer: torch.Size([1, 75, 75, 128]), relpos: torch.Size([1, 75, 75, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50:   0%|          | 0/731 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: torch.Size([1, 75, 640]), p: torch.Size([1, 75, 75, 128])\n",
      "torch.Size([75, 3])\n",
      "ns: torch.Size([75, 640]), nv: torch.Size([75, 3]), es: torch.Size([99, 32]), ev: torch.Size([99, 3])\n",
      "--- INSIDE _make_graph (End) ---\n",
      "    Output edge_feats keys: ['0', '1']\n",
      "    Output edge_feats has NO key '0'\n",
      "  Output data.edge_index shape: torch.Size([2, 99])\n",
      "PRINTING KEYS::::--------\n",
      "nodefeats keys: ['0', '1'], edgefeats keys: ['0', '1']\n",
      "  Verifying shapes before block call...\n",
      "     Shapes verified.\n",
      "\n",
      "--- Entering SE3 Sequential Block ---\n",
      "  Input to Module 0 (AttentionBlockSE3):\n",
      "    features['0'].shape: torch.Size([198, 1, 640])\n",
      "    features['1'].shape: torch.Size([198, 3, 1])\n",
      "Warning: Skipping edge feature concatenation for degree 0 due to shape mismatch.\n",
      "DEBUG Concat (deg=1): Node Shape torch.Size([198, 3, 1]) + Edge Shape torch.Size([198, 3, 1]) -> Result Shape torch.Size([198, 3, 2])\n",
      "ConvSE3FuseLevel: ConvSE3FuseLevel.NONE\n",
      "--- INSIDE RadialProfile.forward ---\n",
      "    Received features shape: torch.Size([198, 33]), size: 6534\n",
      "Outputting shape: torch.Size([198, 81920]).\n",
      "--- INSIDE RadialProfile.forward ---\n",
      "    Received features shape: torch.Size([198, 33]), size: 6534\n",
      "Outputting shape: torch.Size([198, 256]).\n",
      "------DEBUG VERSFORWARD------\n",
      "features passed in shape: torch.Size([198, 3, 2])\n",
      "basis: torch.Size([198, 3, 1, 1])\n",
      "dim in: 3\n",
      "        radial_output.shape = torch.Size([198, 256])\n",
      "        expected_radial_numel = 50688 (based on edges=198, C_out=128, C_in=2, F=1)\n",
      "--- INSIDE RadialProfile.forward ---\n",
      "    Received features shape: torch.Size([198, 33]), size: 6534\n",
      "Outputting shape: torch.Size([198, 81920]).\n",
      "------DEBUG VERSFORWARD------\n",
      "features passed in shape: torch.Size([198, 1, 640])\n",
      "basis: torch.Size([198, 1, 1, 3])\n",
      "dim in: 1\n",
      "        radial_output.shape = torch.Size([198, 81920])\n",
      "        expected_radial_numel = 16220160 (based on edges=198, C_out=128, C_in=640, F=1)\n",
      "--- INSIDE RadialProfile.forward ---\n",
      "    Received features shape: torch.Size([198, 33]), size: 6534\n",
      "Outputting shape: torch.Size([198, 768]).\n",
      "------DEBUG VERSFORWARD------\n",
      "features passed in shape: torch.Size([198, 3, 2])\n",
      "basis: torch.Size([198, 3, 3, 3])\n",
      "dim in: 3\n",
      "        radial_output.shape = torch.Size([198, 768])\n",
      "        expected_radial_numel = 152064 (based on edges=198, C_out=128, C_in=2, F=3)\n",
      "--- INSIDE RadialProfile.forward ---\n",
      "    Received features shape: torch.Size([198, 33]), size: 6534\n",
      "Outputting shape: torch.Size([198, 20480]).\n",
      "------DEBUG VERSFORWARD------\n",
      "features passed in shape: torch.Size([198, 1, 640])\n",
      "basis: torch.Size([198, 1, 1, 5])\n",
      "dim in: 1\n",
      "        radial_output.shape = torch.Size([198, 20480])\n",
      "        expected_radial_numel = 4055040 (based on edges=198, C_out=32, C_in=640, F=1)\n",
      "--- INSIDE RadialProfile.forward ---\n",
      "    Received features shape: torch.Size([198, 33]), size: 6534\n",
      "Outputting shape: torch.Size([198, 192]).\n",
      "------DEBUG VERSFORWARD------\n",
      "features passed in shape: torch.Size([198, 3, 2])\n",
      "basis: torch.Size([198, 3, 3, 5])\n",
      "dim in: 3\n",
      "        radial_output.shape = torch.Size([198, 192])\n",
      "        expected_radial_numel = 38016 (based on edges=198, C_out=32, C_in=2, F=3)\n",
      "\n",
      "--- Patched LinearSE3.forward (Einsum) ---\n",
      "  Input features keys: ['0', '1']\n",
      "    features['0'].shape: torch.Size([198, 1, 640]), Has NaN: False, Has Inf: False\n",
      "    features['1'].shape: torch.Size([198, 3, 1]), Has NaN: False, Has Inf: False\n",
      "\n",
      "  Weights keys: ['0', '1']\n",
      "    self.weights['0'].shape: torch.Size([64, 640])\n",
      "    self.weights['1'].shape: torch.Size([64, 1])\n",
      "------------------------------\n",
      "  Processing degree '0':\n",
      "      Weight shape (oi): torch.Size([64, 640])\n",
      "      Feature shape (ndi): torch.Size([198, 1, 640])\n",
      "    Attempting einsum: torch.einsum('ndi,oi->ndo', features['0'], self.weights['0'])\n",
      "      -> Success! Output shape: torch.Size([198, 1, 64])\n",
      "  Processing degree '1':\n",
      "      Weight shape (oi): torch.Size([64, 1])\n",
      "      Feature shape (ndi): torch.Size([198, 3, 1])\n",
      "    Attempting einsum: torch.einsum('ndi,oi->ndo', features['1'], self.weights['1'])\n",
      "      -> Success! Output shape: torch.Size([198, 3, 64])\n",
      "--- End Patched LinearSE3.forward (Einsum) ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "DGLError",
     "evalue": "[04:20:50] /opt/dgl/src/array/./check.h:51: Check failed: gdim[uev_idx[i]] == arrays[i]->shape[0] (75 vs. 198) : Expect E_data to have size 75 on the first dimension, but got 198\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(+0x7c6d31) [0x7f02c81c6d31]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(dgl::aten::CheckShape(std::vector<unsigned long, std::allocator<unsigned long> > const&, std::vector<int, std::allocator<int> > const&, std::vector<dgl::runtime::NDArray, std::allocator<dgl::runtime::NDArray> > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)+0x35b) [0x7f02c81e9e6b]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(+0x7e4aaf) [0x7f02c81e4aaf]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(+0x7e4d61) [0x7f02c81e4d61]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(DGLFuncCall+0x4c) [0x7f02c8224b9c]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/dgl/_ffi/_cy3/core.cpython-311-x86_64-linux-gnu.so(+0x1ba94) [0x7f033481ba94]\n  [bt] (6) /usr/local/lib/python3.11/dist-packages/dgl/_ffi/_cy3/core.cpython-311-x86_64-linux-gnu.so(+0x1bdff) [0x7f033481bdff]\n  [bt] (7) /usr/local/bin/python3(_PyObject_MakeTpCall+0x28c) [0x52edac]\n  [bt] (8) /usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6bd) [0x53cf5d]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     best_loss, best_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# or config[\"epochs\"]\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcos_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# or config[\"cos_epoch\"]\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, val_dl, epochs, cos_epoch, lr, clip)\u001b[0m\n\u001b[1;32m     31\u001b[0m start_forward \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Remove autocast\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m pred_xyz \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     36\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     37\u001b[0m forward_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_forward\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 167\u001b[0m, in \u001b[0;36mChocolateNet.forward\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m    165\u001b[0m xyz_init \u001b[38;5;241m=\u001b[39m init_coords_from_sequence(sequence, bppm_raw)\n\u001b[1;32m    166\u001b[0m seq_rep, pair_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_transformer(fm_emb, pair_embedding)\n\u001b[0;32m--> 167\u001b[0m xyz_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mse3_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbppm_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyz_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrbf_mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrbf_sigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresh\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xyz_pred\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 218\u001b[0m, in \u001b[0;36mSE3FormerBlocks.forward\u001b[0;34m(self, seq_rep, bppm, xyz_init, rbf_mu, rbf_sigma, thresh)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     Shapes verified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# 6. Call the SE3Transformer block\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m out_feats \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_feats_dgl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass features indexed by source nodes & correctly shaped\u001b[39;49;00m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_feats_dgl\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# 7. Process Output\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m out_feats:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/SE3Transformer/se3_transformer/model/transformer.py:153\u001b[0m, in \u001b[0;36mSE3Transformer.forward\u001b[0;34m(self, graph, node_feats, edge_feats, basis)\u001b[0m\n\u001b[1;32m    148\u001b[0m basis \u001b[38;5;241m=\u001b[39m update_basis_with_fused(basis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_degree, use_pad_trick\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_cores \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory,\n\u001b[1;32m    149\u001b[0m                                 fully_fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuse_level \u001b[38;5;241m==\u001b[39m ConvSE3FuseLevel\u001b[38;5;241m.\u001b[39mFULL)\n\u001b[1;32m    151\u001b[0m edge_feats \u001b[38;5;241m=\u001b[39m get_populated_edge_features(graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrel_pos\u001b[39m\u001b[38;5;124m'\u001b[39m], edge_feats)\n\u001b[0;32m--> 153\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_module(node_feats, graph\u001b[38;5;241m=\u001b[39mgraph)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 14\u001b[0m, in \u001b[0;36mfixed_sequential_forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Should likely be a dict\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Call the module\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Output from Module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(module)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/SE3Transformer/se3_transformer/model/layers/attention.py:164\u001b[0m, in \u001b[0;36mAttentionBlockSE3.forward\u001b[0;34m(self, node_features, edge_features, graph, basis)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nvtx_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    162\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_query(node_features)\n\u001b[0;32m--> 164\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m z_concat \u001b[38;5;241m=\u001b[39m aggregate_residual(node_features, z, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject(z_concat)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/SE3Transformer/se3_transformer/model/layers/attention.py:81\u001b[0m, in \u001b[0;36mAttentionSE3.forward\u001b[0;34m(self, value, key, query, graph)\u001b[0m\n\u001b[1;32m     77\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_fiber\u001b[38;5;241m.\u001b[39mto_attention_heads(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nvtx_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention dot product + softmax\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Compute attention weights (softmax of inner product between key and query)\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     edge_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me_dot_v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m     edge_weights \u001b[38;5;241m=\u001b[39m edge_weights \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_fiber\u001b[38;5;241m.\u001b[39mnum_features)\n\u001b[1;32m     83\u001b[0m     edge_weights \u001b[38;5;241m=\u001b[39m edge_softmax(graph, edge_weights)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/dgl/ops/sddmm.py:137\u001b[0m, in \u001b[0;36m_gen_sddmm_func.<locals>.func\u001b[0;34m(g, x, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(g, x, y):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgsddmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlhs_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlhs_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrhs_target\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/dgl/ops/sddmm.py:78\u001b[0m, in \u001b[0;36mgsddmm\u001b[0;34m(g, op, lhs_data, rhs_data, lhs_target, rhs_target)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_lhs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_rhs\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     77\u001b[0m         lhs_data, rhs_data \u001b[38;5;241m=\u001b[39m reshape_lhs_rhs(lhs_data, rhs_data)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgsddmm_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlhs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlhs_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_target\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_lhs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/dgl/backend/pytorch/sparse.py:1046\u001b[0m, in \u001b[0;36mgsddmm\u001b[0;34m(gidx, op, lhs_data, rhs_data, lhs_target, rhs_target)\u001b[0m\n\u001b[1;32m   1042\u001b[0m args \u001b[38;5;241m=\u001b[39m _cast_if_autocast_enabled(\n\u001b[1;32m   1043\u001b[0m     gidx, op, lhs_data, rhs_data, lhs_target, rhs_target\n\u001b[1;32m   1044\u001b[0m )\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _disable_autocast_if_enabled():\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGSDDMM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/dgl/backend/pytorch/sparse.py:446\u001b[0m, in \u001b[0;36mGSDDMM.forward\u001b[0;34m(ctx, gidx, op, X, Y, lhs_target, rhs_target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, gidx, op, X, Y, lhs_target, rhs_target):\n\u001b[0;32m--> 446\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m_gsddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlhs_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     X_shape \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     Y_shape \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/dgl/_sparse_ops.py:554\u001b[0m, in \u001b[0;36m_gsddmm\u001b[0;34m(gidx, op, lhs, rhs, lhs_target, rhs_target)\u001b[0m\n\u001b[1;32m    552\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mempty(out_shp, dtype, ctx)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gidx\u001b[38;5;241m.\u001b[39mnum_edges(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 554\u001b[0m     \u001b[43m_CAPI_DGLKernelSDDMM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgidx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_dgl_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_lhs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_dgl_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_rhs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_dgl_nd_for_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlhs_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrhs_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (expand_lhs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_lhs) \u001b[38;5;129;01mand\u001b[39;00m (expand_rhs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_rhs):\n\u001b[1;32m    564\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msqueeze(out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./function.pxi:295\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.FunctionBase.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./function.pxi:241\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDGLError\u001b[0m: [04:20:50] /opt/dgl/src/array/./check.h:51: Check failed: gdim[uev_idx[i]] == arrays[i]->shape[0] (75 vs. 198) : Expect E_data to have size 75 on the first dimension, but got 198\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(+0x7c6d31) [0x7f02c81c6d31]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(dgl::aten::CheckShape(std::vector<unsigned long, std::allocator<unsigned long> > const&, std::vector<int, std::allocator<int> > const&, std::vector<dgl::runtime::NDArray, std::allocator<dgl::runtime::NDArray> > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)+0x35b) [0x7f02c81e9e6b]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(+0x7e4aaf) [0x7f02c81e4aaf]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(+0x7e4d61) [0x7f02c81e4d61]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/dgl/libdgl.so(DGLFuncCall+0x4c) [0x7f02c8224b9c]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/dgl/_ffi/_cy3/core.cpython-311-x86_64-linux-gnu.so(+0x1ba94) [0x7f033481ba94]\n  [bt] (6) /usr/local/lib/python3.11/dist-packages/dgl/_ffi/_cy3/core.cpython-311-x86_64-linux-gnu.so(+0x1bdff) [0x7f033481bdff]\n  [bt] (7) /usr/local/bin/python3(_PyObject_MakeTpCall+0x28c) [0x52edac]\n  [bt] (8) /usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6bd) [0x53cf5d]\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_loss, best_predictions = train_model(\n",
    "        model=model,\n",
    "        train_dl=train_loader,\n",
    "        val_dl=val_loader,\n",
    "        epochs=50,         # or config[\"epochs\"]\n",
    "        cos_epoch=35,      # or config[\"cos_epoch\"]\n",
    "        lr=3e-4,\n",
    "        clip=1\n",
    "    )\n",
    "    print(f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-01T02:54:57.633739Z",
     "iopub.status.idle": "2025-05-01T02:54:57.634017Z",
     "shell.execute_reply": "2025-05-01T02:54:57.633937Z",
     "shell.execute_reply.started": "2025-05-01T02:54:57.633926Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y dgl\n",
    "# !pip install --pre dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-01T02:54:57.634631Z",
     "iopub.status.idle": "2025-05-01T02:54:57.634964Z",
     "shell.execute_reply": "2025-05-01T02:54:57.634863Z",
     "shell.execute_reply.started": "2025-05-01T02:54:57.634851Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(config[\"test_data_path\"]) # target_id,sequence,temporal_cutoff,description,all_sequences\n",
    "print(test_df.head(10))\n",
    "test_model = FinetunedRibonanzaNet(model_cfg, pretrained_state=2).cuda()\n",
    "test_model.eval()\n",
    "\n",
    "submission_rows = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running inference\"):\n",
    "    seq_id = row[\"target_id\"]\n",
    "    seq = row[\"sequence\"]\n",
    "    \n",
    "    token_map = {'A': 0, 'C': 1, 'U': 2, 'G': 3}\n",
    "    token_ids = torch.tensor([token_map[c] for c in seq], dtype=torch.long).unsqueeze(0).cuda()  # shape (1, L)\n",
    "    mask = torch.ones_like(token_ids).cuda()  # or derive if needed\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):  # generate 5 predictions\n",
    "            pred_xyz = test_model(token_ids, mask).squeeze(0).cpu().numpy()  # shape (L, 3)\n",
    "            preds.append(pred_xyz)\n",
    "\n",
    "    preds = np.stack(preds, axis=0)  # shape (5, L, 3)\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        resname = seq[i]\n",
    "        resid = i + 1\n",
    "        flat_xyz = preds[:, i, :].flatten()  # (x1,y1,z1,...,x5,y5,z5)\n",
    "        row = [f\"{seq_id}\", resname, resid] + flat_xyz.tolist()\n",
    "        submission_rows.append(row)\n",
    "\n",
    "# Save to CSV\n",
    "columns = [\"ID\", \"resname\", \"resid\"] + [f\"{axis}_{i+1}\" for i in range(5) for axis in [\"x\", \"y\", \"z\"]]\n",
    "submission = pd.DataFrame(submission_rows, columns=columns)\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Inference complete! Saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11403143,
     "sourceId": 87793,
     "sourceType": "competition"
    },
    {
     "datasetId": 4299272,
     "sourceId": 7639698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4459124,
     "sourceId": 8318191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
